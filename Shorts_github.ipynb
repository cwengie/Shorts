{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pandas_market_calendars as mcal\n",
    "import holidays\n",
    "from pandas.tseries.offsets import BDay\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import stata_setup\n",
    "\n",
    "working_folder = \"D:/FINRA_Working_Data\"\n",
    "monthly_data = \"E:/FINRA_Data_by_Month\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:17:09.969361800Z",
     "start_time": "2024-08-24T21:17:08.531410300Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Manually downloaded short sale data from FINRA website 4/28/24\n",
    "# FNRA - ADF\n",
    "# FNSQ - Nasdaq Carteret\n",
    "# FNQC - Nasdaq Chicago\n",
    "# FNYX - NYSE "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:33:37.652000800Z",
     "start_time": "2024-05-22T09:33:37.648309Z"
    }
   },
   "id": "c47b744f1c2d3e3c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FINRA Daily Shorting Volume Wrangling\n",
    "Steps for redoing:\n",
    "1. Get all daily volume files \n",
    "2. Delete 20100915 \n",
    "3. Clean up data types, consolidate days before 8/1/18\n",
    "4. Add ShortExemptVolume for days before 2/28/11\n",
    "5. Combine all days into one file "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68214cc0a0b2f2f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get daily shorting files from FINRA website (webscraped since API only returns data from past 1 year)\n",
    "\n",
    "\n",
    "# Set date ranges (consolidated files available from 8/1/18)\n",
    "start_date_1 = datetime(2010, 1, 1)\n",
    "end_date_1 = datetime(2018, 7, 31)\n",
    "start_date_2 = datetime(2018, 8, 1)\n",
    "end_date_2 = datetime(2024, 8, 9)\n",
    "\n",
    "# File suffixes\n",
    "facilities_1 = [\"FNRA\", \"FNSQ\", \"FNYX\"]\n",
    "facility_2 = \"CNMS\"\n",
    "\n",
    "reports_dir = \"D:/FINRA_Volume/Raw_Daily_Volume\"\n",
    "# os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Function to query data and save to files with retry logic\n",
    "def fetch_and_save_data(date_str, facility, retries=5):\n",
    "    url = f\"https://cdn.finra.org/equity/regsho/daily/{facility}shvol{date_str}.txt\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # Query the URL with a timeout\n",
    "            response = requests.get(url, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                #cleaned_content = response.text.replace('\\n', '')\n",
    "\n",
    "                filename = f\"{date_str}_{facility}.txt\"\n",
    "                file_path = os.path.join(reports_dir, filename)\n",
    "\n",
    "                with open(file_path, 'w') as file:\n",
    "                    #file.write(cleaned_content)\n",
    "                    file.write(response.text)\n",
    "\n",
    "                return  # Exit the function if successful\n",
    "\n",
    "            else:\n",
    "                #print(f\"Failed to retrieve data for {date_str} and {facility}: {response.status_code}\")\n",
    "                return  # Exit the function if failure\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error occurred for {date_str} and {facility}: {e}\")\n",
    "            attempt += 1\n",
    "            if attempt == retries:\n",
    "                print(f\"Final retry FAILED... ({attempt}/{retries})\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "\n",
    "# Function to process a range of dates and facilities in parallel\n",
    "def process_dates_in_parallel(start_date, end_date, facilities):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y%m%d')\n",
    "            for facility in facilities:\n",
    "                futures.append(executor.submit(fetch_and_save_data, date_str, facility))\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "# Process the first date range (2009-2018) in parallel\n",
    "process_dates_in_parallel(start_date_1, end_date_1, facilities_1)\n",
    "\n",
    "# Process the second date range (2018-2024) in parallel\n",
    "process_dates_in_parallel(start_date_2, end_date_2, [facility_2])\n",
    "\n",
    "print(\"Script finished running.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f8abf319f3781a2",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Clean daily volume files\n",
    "\n",
    "\n",
    "# Need to define folder_path, which should contain raw files\n",
    "input_folder = \"D:/FINRA_Volume/Raw_Daily_Volume\"\n",
    "output_folder = \"D:/FINRA_Volume/Consolidated_Daily\"\n",
    "\n",
    "# Consolidate days before 8/1/18\n",
    "# Add ShortExemptVolume for days before 2/28/11\n",
    "            \n",
    "# Loop over each day in 2010-2023\n",
    "for year in range(2010, 2024):\n",
    "    metadata = {}\n",
    "\n",
    "    for month in range(1, 13):\n",
    "            \n",
    "        for day in range(1,32):\n",
    "            \n",
    "            date = f\"{year}{month:02d}{day:02d}\"\n",
    "    \n",
    "            if (year < 2018) | ((year == 2018)& (month <= 7)):\n",
    "                file_list = [file for file in os.listdir(input_folder) if date in file]\n",
    "        \n",
    "                # Combine multiple files per day\n",
    "                if file_list:\n",
    "                \n",
    "                    combined_df = pd.DataFrame()\n",
    "            \n",
    "                    for file in file_list:\n",
    "                        df = pd.read_csv(os.path.join(input_folder, file), delimiter='|', index_col=False)\n",
    "                        df = df.iloc[:-1, :]  # delete check row\n",
    "                        if (year == 2010) | ((year == 2011) & (month == 1)) | ((year == 2011) & (month == 2) & (day <= 27)):\n",
    "                            df.insert(df.columns.get_loc('ShortVolume') + 1, 'ShortExemptVolume', 0)\n",
    "                        if not df.empty and not df.isna().all().all():\n",
    "                            df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d', errors='coerce')\n",
    "                            df = df.dropna(subset=['Date', 'Symbol', 'ShortVolume', 'ShortExemptVolume', 'TotalVolume'])\n",
    "                            combined_df = pd.concat([combined_df, df])\n",
    "    \n",
    "                    combined_df = pd.concat([combined_df, df])\n",
    "            \n",
    "                    combined_df = combined_df.groupby(['Date', 'Symbol'], as_index=False)[['ShortVolume', 'ShortExemptVolume','TotalVolume']].sum()\n",
    "                    \n",
    "                    combined_df = combined_df.sort_values(by=['Symbol']).reset_index(drop=True)\n",
    "                    combined_df.to_csv(output_folder + '/' + date + '.txt', sep='|', index=False)\n",
    "            \n",
    "            else:\n",
    "                file_path = input_folder + f'/{year}{month:02d}{day:02d}_CNMS.txt'\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    df = pd.read_csv(file_path, delimiter='|', index_col=False)\n",
    "                    df = df.iloc[:-1, :]  # delete check row\n",
    "                    df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d', errors='coerce')\n",
    "                    df = df.dropna(subset=['Date', 'Symbol', 'ShortVolume', 'ShortExemptVolume', 'TotalVolume'])\n",
    "                    df.to_csv(output_folder + '/' + f'/{year}{month:02d}{day:02d}.txt', sep='|', index=False)\n",
    "            print(date)\n",
    "\n",
    "# Combine daily files into one\n",
    "dfs = []\n",
    "\n",
    "# Loop through all text files in the folder\n",
    "for file in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "    df = pd.read_csv(file, sep='|', index_col=False, parse_dates=['Date'], date_format='%Y-%m-%d') \n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.sort_values(by=['Date', 'Symbol'], inplace=True)\n",
    "\n",
    "combined_df.to_pickle('D:/FINRA_Volume/daily_volume.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T17:20:01.700299100Z",
     "start_time": "2024-08-11T17:18:17.289800800Z"
    }
   },
   "id": "d84e444ed24ae978",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## FINRA Monthly Detailed Shorting Data Wrangling ##\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13ca43bb300e8dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create monthly files from raw files\n",
    "\n",
    "# Need to define folder_path, which should contain raw files\n",
    "folder_path = \"D:/FINRA_Raw_Data\"\n",
    "\n",
    "# Loop over each month in 2010-2023\n",
    "for year in range(2010, 2024):\n",
    "    metadata = {}\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "\n",
    "        year_month = f\"{year}{month:02d}\"\n",
    "\n",
    "        file_list = [file for file in os.listdir(folder_path) if year_month in file]\n",
    "        \n",
    "        # Combine files \n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for file in file_list:\n",
    "            df = pd.read_csv(os.path.join(folder_path, file), delimiter='|', index_col=False, parse_dates=['Date'], date_format='ISO8601')\n",
    "            df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time\n",
    "            df = df.iloc[:-1, :-1]  # delete check row and empty last column\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "        # Sort the DataFrame by symbol, date and time\n",
    "        combined_df = combined_df.sort_values(by=['Symbol','Date', 'Time']).reset_index(drop=True)\n",
    "\n",
    "        # Store metadata\n",
    "        month_metadata = {}\n",
    "        month_metadata['num_rows'] = combined_df.shape[0]\n",
    "        month_metadata['unique_market_centers'] = combined_df['MarketCenter'].unique().tolist()\n",
    "        month_metadata['min_date'] = combined_df['Date'].min()\n",
    "        month_metadata['max_date'] = combined_df['Date'].max()\n",
    "        metadata[year_month] = month_metadata\n",
    "\n",
    "        # Save the concatenated data as a .pkl file\n",
    "        combined_df.to_pickle(monthly_data + '/' + year_month + '.pkl')\n",
    "        print(year_month)\n",
    "\n",
    "for year in range(2010, 2024):# Save metadata\n",
    "    meta = pd.DataFrame(metadata).T\n",
    "    meta = meta.rename_axis('Month').reset_index()  # Rename the first column heading as 'Month'\n",
    "    meta.to_csv(working_folder+'/Metadata_'+f\"{year}\"+'.txt', sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54e0ca1591b280d0",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata= {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T19:27:15.394877400Z",
     "start_time": "2024-05-15T19:27:15.381780900Z"
    }
   },
   "id": "8fde0a9901541426",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Update metadata\n",
    "\n",
    "month1 = pd.read_pickle(monthly_data+'/202302.pkl')\n",
    "month1['Date'] = pd.to_datetime(month1['Date'], format='ISO8601')\n",
    "month1['Time'] = pd.to_datetime(month1['Time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "metadata['202302'] = {'num_rows': month1.shape[0],\n",
    "                      'unique_market_centers': month1['MarketCenter'].unique().tolist(),\n",
    "                      'min_date': month1['Date'].min(),\n",
    "                      'max_date': month1['Date'].max()}\n",
    "\n",
    "meta = pd.DataFrame(metadata).T\n",
    "meta = meta.rename_axis('Month').reset_index()  # Rename the first column heading as 'Month'\n",
    "meta.to_csv(working_folder+'/Metadata_fillinpartial.txt', sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d64bc94ea6148a",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create 1-minute or 5-minute aggregated files from monthly files\n",
    "\n",
    "agg_level = '5min'\n",
    "#agg_level = '1min'\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "for year in range(2010, 2024):\n",
    "    for month in range(1, 13):\n",
    "        #if year == 2010 and month == 1:\n",
    "            #continue  # Skip the first month of 2010\n",
    "            \n",
    "        file_path = f'{monthly_data}/{year}{month:02d}.pkl'\n",
    "\n",
    "        try:\n",
    "            raw_daily = pd.read_pickle(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f'{monthly_data}/{year}{month:02d}.pkl' + ' not found')\n",
    "            continue  # Skip missing files\n",
    "\n",
    "        # Round the timestamp down to agg_level\n",
    "        raw_daily['Time'] = pd.to_datetime(raw_daily['Time'], format='%H:%M:%S')\n",
    "        raw_daily['Time'] = raw_daily['Time'].dt.floor(agg_level).dt.time\n",
    "\n",
    "        con.register('raw_daily', raw_daily)\n",
    "\n",
    "        # Aggregate shorts where ShortType is 'S'\n",
    "        agg_data_s = con.execute('''\n",
    "            SELECT Symbol, Date, Time, SUM(Size) AS Shorts\n",
    "            FROM raw_daily\n",
    "            WHERE ShortType = 'S'\n",
    "            GROUP BY Symbol, Date, Time\n",
    "        ''').fetchdf()\n",
    "\n",
    "        # Aggregate shorts where ShortType is not 'S'\n",
    "        agg_data_exempt = con.execute('''\n",
    "            SELECT Symbol, Date, Time, SUM(Size) AS ExemptShorts\n",
    "            FROM raw_daily\n",
    "            WHERE ShortType != 'S'\n",
    "            GROUP BY Symbol, Date, Time\n",
    "        ''').fetchdf()\n",
    "\n",
    "        # Merge the results\n",
    "        agg_data = con.execute('''\n",
    "            SELECT \n",
    "                a.Symbol, a.Date, a.Time,\n",
    "                COALESCE(a.Shorts, 0) AS Shorts,\n",
    "                COALESCE(b.ExemptShorts, 0) AS ExemptShorts\n",
    "            FROM agg_data_s a\n",
    "            FULL OUTER JOIN agg_data_exempt b\n",
    "            ON a.Symbol = b.Symbol AND a.Date = b.Date AND a.Time = b.Time\n",
    "        ''').fetchdf()\n",
    "\n",
    "        agg_data.to_pickle(f'{working_folder}/agg_{year}{month:02d}_{agg_level}.pkl')\n",
    "        #print(f'{year} {month} max exempt shorts: {agg_data[\"ExemptShorts\"].max()}')\n",
    "        print(f'{year} {month} 5min agg done')\n",
    "\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9010f718db227e6",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create coarser aggregated files from 5-minute aggregated files\n",
    "# Daily, 1 hour or 30 min\n",
    "\n",
    "for year in range(2011, 2024):\n",
    "    annual_agg_data = pd.DataFrame()  # Initialize a new DataFrame for each year\n",
    "\n",
    "    for month in range(1, 13):\n",
    "\n",
    "        file_path = f'{working_folder}/FINRA_5min_Agg/agg_{year}{month:02d}_5min.pkl'\n",
    "\n",
    "        try:\n",
    "            monthly_data = pd.read_pickle(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f'{file_path} not found')\n",
    "            continue  # Skip missing files\n",
    "\n",
    "        # monthly_data['Hour'] = pd.to_datetime(monthly_data['Time'], format='%H:%M:%S').dt.hour\n",
    "        #monthly_data['HalfHour'] = pd.to_datetime(monthly_data['Time'], format='%H:%M:%S').dt.floor('30min').dt.time\n",
    "\n",
    "        # Aggregate trades and sort\n",
    "        monthly_data = monthly_data.groupby(['Symbol', 'Date']).agg({ 'Shorts': 'sum', 'ExemptShorts': 'sum'}).reset_index()\n",
    "        monthly_data = monthly_data.sort_values(by=['Symbol', 'Date']).reset_index(drop=True)\n",
    "        # monthly_data = monthly_data.groupby(['Symbol', 'Date', 'Hour']).agg({ 'Shorts': 'sum', 'ExemptShorts': 'sum'}).reset_index()\n",
    "        # monthly_data = monthly_data.sort_values(by=['Symbol', 'Date', 'Hour']).reset_index(drop=True)\n",
    "        # monthly_data = monthly_data.groupby(['Symbol', 'Date', 'HalfHour']).agg({ 'Shorts': 'sum', 'ExemptShorts': 'sum'}).reset_index()\n",
    "        # monthly_data = monthly_data.sort_values(by=['Symbol', 'Date', 'HalfHour']).reset_index(drop=True)\n",
    "\n",
    "        # Append the monthly aggregated data to the annual DataFrame\n",
    "        annual_agg_data = pd.concat([annual_agg_data, monthly_data], ignore_index=True)\n",
    "\n",
    "    # Save the aggregated data for the entire year\n",
    "    annual_agg_data.to_pickle(f'D:/FINRA_Daily_Agg/agg_{year}.pkl')\n",
    "    print(f'{year} data aggregated by day')\n",
    "    # annual_agg_data.to_pickle(f'{working_folder}/FINRA_1hr_Agg/agg_{year}_1hr.pkl')\n",
    "    # print(f'{year} data aggregated by hour')\n",
    "    # annual_agg_data.to_pickle(f'{working_folder}/FINRA_30min_Agg/agg_{year}_30min.pkl')\n",
    "    # print(f'{year} data aggregated by half hour')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29e89ef2efda6858",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bbcf3ae5d0a36cdf",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T21:34:57.607647500Z",
     "start_time": "2024-08-12T21:34:55.623540500Z"
    }
   },
   "id": "39967afbb04e5bce",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f30e96c4b0b7c550",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7fbbdaf16d24f43e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c815ff019c55c81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5eff57e88eb82d9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# IBES Data Cleaning + Create Table of EAs and Relevant Trading Dates\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "576cf830cab4a0fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Clean the I/B/E/S data downloaded from WRDS 08/23/2024\n",
    "# I/B/E/S Unadjusted Surprise History for fiscal periods ending 2009-01 to 2024-01 (only has IBES ticker and official ticker)\n",
    "\n",
    "\n",
    "surprise = pd.read_csv(working_folder + '/statsum_unadjsurp_original.csv', delimiter = ',')\n",
    "surprise = surprise.drop(columns=['MEASURE', 'FISCALP', 'PYEAR', 'PMON', 'actual'])\n",
    "surprise = surprise.dropna()\n",
    "\n",
    "# Drop OFTIC-anndats duplicates\n",
    "duplicates = surprise.duplicated(subset=['TICKER', 'anndats'], keep=False)\n",
    "surprise = surprise[~duplicates]\n",
    "\n",
    "# Clean IBES unadjusted actuals\n",
    "actuals = pd.read_csv(working_folder + '/actual_unadj_original.csv', delimiter = ',')\n",
    "actuals = actuals.drop(columns=['MEASURE', 'PDICITY'])\n",
    "actuals = actuals.dropna()\n",
    "\n",
    "# Combine date and time columns into a single datetime column\n",
    "actuals['ANNOUNCE'] = pd.to_datetime(actuals['ANNDATS'] + ' ' + actuals['ANNTIMS'], format='%Y-%m-%d %H:%M:%S')\n",
    "actuals['ACTIVATE'] = pd.to_datetime(actuals['ACTDATS'] + ' ' + actuals['ACTTIMS'], format='%Y-%m-%d %H:%M:%S')\n",
    "actuals['PENDS'], actuals['ACTDATS'], actuals['ACTTIMS'] = pd.to_datetime(actuals['PENDS'], format='%Y-%m-%d'), pd.to_datetime(actuals['ACTDATS'], format='%Y-%m-%d'), pd.to_datetime(actuals['ACTTIMS'], format='%H:%M:%S')\n",
    "\n",
    "# Keep the row with the latest activation for a given CUSIP-PENDS combination\n",
    "actuals = actuals.sort_values(by=['CUSIP', 'PENDS', 'ACTIVATE'], ascending=[True, True, True]).groupby(['CUSIP', 'PENDS']).last().reset_index()\n",
    "\n",
    "# Drop rows that have earnings announcements earlier than the fiscal period end\n",
    "actuals = actuals[actuals['ANNOUNCE'].dt.date >=actuals['PENDS']]\n",
    "\n",
    "# Merge firm identifiers, actual EPS, and announcement time from IBES unadjusted actuals\n",
    "con = duckdb.connect()\n",
    "con.register('surprise', surprise)\n",
    "con.register('actuals', actuals)\n",
    "surprise = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        surprise.*, \n",
    "        actuals.CUSIP, actuals.PENDS , actuals.CNAME , actuals.ANNTIMS, actuals.VALUE\n",
    "    FROM \n",
    "        surprise \n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "            TICKER, CUSIP, PENDS, CNAME, ANNDATS, ANNTIMS, VALUE\n",
    "        FROM actuals\n",
    "    ) AS actuals\n",
    "    ON \n",
    "        surprise.TICKER = actuals.TICKER\n",
    "        AND surprise.anndats = actuals.ANNDATS\n",
    "    ORDER BY \n",
    "        surprise.OFTIC, \n",
    "        surprise.anndats\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "surprise['ANNOUNCE'] = pd.to_datetime(surprise['anndats'] + ' ' + surprise['ANNTIMS'], format='%Y-%m-%d %H:%M:%S')\n",
    "surprise['ANNOUNCE'] = surprise['ANNOUNCE'].dt.floor('5min')\n",
    "\n",
    "# Keep announcements Jan. 5 2010 - Dec. 28 2023 \n",
    "surprise = surprise[(surprise['ANNOUNCE'] >= '2010-01-05') & (surprise['ANNOUNCE'] <= '2023-12-28')]\n",
    "\n",
    "# Calculate surprise\n",
    "surprise['SURPRISE'] = surprise['VALUE'] - surprise['surpmean']\n",
    "\n",
    "surprise = surprise.sort_values(by=['OFTIC', 'ANNOUNCE'], ascending=[True, True]).reset_index(drop=True)\n",
    "surprise = surprise.drop(columns=['anndats', 'ANNTIMS', 'TICKER'])\n",
    "surprise = surprise[['CUSIP', 'OFTIC', 'CNAME', 'PENDS', 'ANNOUNCE', 'SURPRISE']] # Drop and reorder columns\n",
    "surprise.to_csv(working_folder + '/statsum_unadjsurp_clean.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:21:04.712905400Z",
     "start_time": "2024-08-24T21:21:00.102749400Z"
    }
   },
   "id": "cb3e69756eb410e3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ea_dates = pd.read_csv(working_folder + '/statsum_unadjsurp_clean.csv', parse_dates=['PENDS', 'ANNOUNCE'], date_format='ISO8601')\n",
    "ea_dates = ea_dates[['CUSIP', 'OFTIC', 'CNAME', 'ANNOUNCE', 'SURPRISE']]\n",
    "ea_dates['ANNOUNCE'] = pd.to_datetime(ea_dates['ANNOUNCE']).dt.tz_localize(None)\n",
    "\n",
    "# Identify positive and negative surprises\n",
    "ea_dates['POS_SURPRISE'] = ea_dates['SURPRISE'] > 0\n",
    "\n",
    "# Identify pre-open and post-close announcements\n",
    "ea_dates['pre_open'] = False\n",
    "ea_dates['post_close'] = False\n",
    "\n",
    "announce_time = ea_dates['ANNOUNCE'].dt.time\n",
    "\n",
    "ea_dates.loc[announce_time < pd.to_datetime('09:30:00').time(), 'pre_open'] = True\n",
    "ea_dates.loc[announce_time >= pd.to_datetime('16:00:00').time(), 'post_close'] = True\n",
    "\n",
    "# Drop EAs within trading hours\n",
    "ea_dates = ea_dates[ea_dates['pre_open'] | ea_dates['post_close']]\n",
    "\n",
    "# Define trading calendar\n",
    "nyse_holidays = holidays.NYSE()\n",
    "\n",
    "# Function to get previous and next trading days\n",
    "def get_surr_trading_days(date):\n",
    "\n",
    "    one_day = pd.Timedelta(days=1)\n",
    "\n",
    "    def get_previous_trading_day(day, offset=1):\n",
    "        for _ in range(offset):\n",
    "            day -= one_day\n",
    "            while day.weekday() >= 5 or day in nyse_holidays: # Skip weekends and holidays\n",
    "                day -= one_day\n",
    "        return day\n",
    "\n",
    "    def get_next_trading_day(day, offset=1):\n",
    "        for _ in range(offset):\n",
    "            day += one_day\n",
    "            while day.weekday() >= 5 or day in nyse_holidays: # Skip weekends and holidays\n",
    "                day += one_day\n",
    "        return day\n",
    "\n",
    "    # post-close announcements\n",
    "    if date.time() >= pd.to_datetime('12:00:00').time():\n",
    "        day_before = date\n",
    "        while day_before.weekday() >= 5 or day_before in nyse_holidays:\n",
    "            day_before -= one_day\n",
    "        day_after = get_next_trading_day(day_before)\n",
    "\n",
    "    # pre-open announcements\n",
    "    else:\n",
    "        day_before = get_previous_trading_day(date)\n",
    "\n",
    "        day_after = date\n",
    "        while day_after.weekday() >= 5 or day_after in nyse_holidays:\n",
    "            day_after += one_day\n",
    "\n",
    "    # Calculate the previous days\n",
    "    day_2_before = get_previous_trading_day(day_before)\n",
    "    day_3_before = get_previous_trading_day(day_2_before)\n",
    "    day_4_before = get_previous_trading_day(day_3_before)\n",
    "    day_5_before = get_previous_trading_day(day_4_before)\n",
    "    day_6_before = get_previous_trading_day(day_5_before)\n",
    "    day_25_before = get_previous_trading_day(day_6_before, offset=19)\n",
    "\n",
    "    day_2_after = get_next_trading_day(day_after)\n",
    "    day_3_after = get_next_trading_day(day_2_after)\n",
    "    day_4_after = get_next_trading_day(day_3_after)\n",
    "    day_5_after = get_next_trading_day(day_4_after)\n",
    "    \n",
    "    return  day_5_after.date(), day_4_after.date(), day_3_after.date(), day_2_after.date(), day_after.date(), day_before.date(), day_2_before.date(), day_3_before.date(), day_4_before.date(), day_5_before.date(), day_6_before.date(), day_25_before.date()\n",
    "\n",
    "ea_dates['day_5_after'], ea_dates['day_4_after'], ea_dates['day_3_after'], ea_dates['day_2_after'], ea_dates['day_after'], ea_dates['day_before'], ea_dates['day_2_before'], ea_dates['day_3_before'], ea_dates['day_4_before'], ea_dates['day_5_before'], ea_dates['day_6_before'], ea_dates['day_25_before'] = zip(*ea_dates['ANNOUNCE'].apply(lambda x: get_surr_trading_days(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:22:23.304103800Z",
     "start_time": "2024-08-24T21:21:04.719180400Z"
    }
   },
   "id": "5839f58b356d8561",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add point-in-tickers, matching on CUSIP using linking table from Algoseek (thanks Matt)\n",
    "\n",
    "link = pd.read_parquet(working_folder + '/crsp_aseek.parquet')\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('ea_dates', ea_dates)\n",
    "con.register('link', link)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ea.CUSIP, lk.permno, lk.permco, lk.ticker as pit_ticker, ea.OFTIC, ea.CNAME, ea.ANNOUNCE, ea.SURPRISE, ea.POS_SURPRISE, ea.pre_open, ea.post_close, ea.day_5_after, ea.day_4_after, ea.day_3_after, ea.day_2_after, ea.day_after, ea.day_before, ea.day_2_before, ea.day_3_before, ea.day_4_before, ea.day_5_before, ea.day_6_before, ea.day_25_before\n",
    "    FROM ea_dates ea\n",
    "    LEFT JOIN (\n",
    "        SELECT cusip, bdate, edate, permno, permco, ticker\n",
    "        FROM link\n",
    "    ) lk\n",
    "    ON ea.CUSIP = substr(lk.cusip, 1, 8)\n",
    "    WHERE ea.ANNOUNCE BETWEEN lk.bdate AND lk.edate \n",
    "    ORDER BY ea.ANNOUNCE, lk.ticker\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "ea_dates = con.execute(query).df()\n",
    "con.close()\n",
    "\n",
    "ea_dates.to_csv(working_folder+ '/ea_dates.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:22:26.107830600Z",
     "start_time": "2024-08-24T21:22:23.305138400Z"
    }
   },
   "id": "b75e8f4c8254b344",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           CUSIP  permno  permco pit_ticker OFTIC           CNAME  \\\n0       83545110   76568   10843       SONC  SONC      SONIC CORP   \n1       03475V10   90179   45192       ANGO  ANGO   ANGIODYNAMICS   \n2       51476610   83148   14374       LNDC  LNDC     LANDEC CORP   \n3       87815510   65752    4402       TISI  TISI        TEAM INC   \n4       61945A10   90386   45695        MOS   MOS   MOSAIC CO/THE   \n...          ...     ...     ...        ...   ...             ...   \n192411  69012T20   16104   55569       OTLK  OTLK         OUTLOOK   \n192412  12482W30   17091   56139       YCBD  YCBD           CBDMD   \n192413  16842Q10   16883   56045       CSSE  CSSE    CHICKEN SOUP   \n192414  L7756P10   21955   58741       PROC  PROC   PROCAPS GROUP   \n192415  10807Q70   92091   52371       BLIN  BLIN  BRIDGELINE DIG   \n\n                  ANNOUNCE  SURPRISE  POS_SURPRISE  pre_open  ...  \\\n0      2010-01-05 16:00:00  -0.03368         False     False  ...   \n1      2010-01-05 16:05:00   0.02000          True     False  ...   \n2      2010-01-05 16:05:00  -0.00333         False     False  ...   \n3      2010-01-05 16:05:00   0.04250          True     False  ...   \n4      2010-01-05 16:15:00  -0.03176         False     False  ...   \n...                    ...       ...           ...       ...  ...   \n192411 2023-12-22 08:05:00   0.00875          True      True  ...   \n192412 2023-12-22 16:00:00  -0.45000         False     False  ...   \n192413 2023-12-22 17:00:00  -0.68600         False     False  ...   \n192414 2023-12-26 17:25:00   0.03000          True     False  ...   \n192415 2023-12-27 16:00:00   0.01333          True     False  ...   \n\n        day_3_after day_2_after  day_after day_before day_2_before  \\\n0        2010-01-08  2010-01-07 2010-01-06 2010-01-05   2010-01-04   \n1        2010-01-08  2010-01-07 2010-01-06 2010-01-05   2010-01-04   \n2        2010-01-08  2010-01-07 2010-01-06 2010-01-05   2010-01-04   \n3        2010-01-08  2010-01-07 2010-01-06 2010-01-05   2010-01-04   \n4        2010-01-08  2010-01-07 2010-01-06 2010-01-05   2010-01-04   \n...             ...         ...        ...        ...          ...   \n192411   2023-12-27  2023-12-26 2023-12-22 2023-12-21   2023-12-20   \n192412   2023-12-28  2023-12-27 2023-12-26 2023-12-22   2023-12-21   \n192413   2023-12-28  2023-12-27 2023-12-26 2023-12-22   2023-12-21   \n192414   2023-12-29  2023-12-28 2023-12-27 2023-12-26   2023-12-22   \n192415   2024-01-02  2023-12-29 2023-12-28 2023-12-27   2023-12-26   \n\n       day_3_before day_4_before day_5_before day_6_before day_25_before  \n0        2009-12-31   2009-12-30   2009-12-29   2009-12-28    2009-11-30  \n1        2009-12-31   2009-12-30   2009-12-29   2009-12-28    2009-11-30  \n2        2009-12-31   2009-12-30   2009-12-29   2009-12-28    2009-11-30  \n3        2009-12-31   2009-12-30   2009-12-29   2009-12-28    2009-11-30  \n4        2009-12-31   2009-12-30   2009-12-29   2009-12-28    2009-11-30  \n...             ...          ...          ...          ...           ...  \n192411   2023-12-19   2023-12-18   2023-12-15   2023-12-14    2023-11-16  \n192412   2023-12-20   2023-12-19   2023-12-18   2023-12-15    2023-11-17  \n192413   2023-12-20   2023-12-19   2023-12-18   2023-12-15    2023-11-17  \n192414   2023-12-21   2023-12-20   2023-12-19   2023-12-18    2023-11-20  \n192415   2023-12-22   2023-12-21   2023-12-20   2023-12-19    2023-11-21  \n\n[192416 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSIP</th>\n      <th>permno</th>\n      <th>permco</th>\n      <th>pit_ticker</th>\n      <th>OFTIC</th>\n      <th>CNAME</th>\n      <th>ANNOUNCE</th>\n      <th>SURPRISE</th>\n      <th>POS_SURPRISE</th>\n      <th>pre_open</th>\n      <th>...</th>\n      <th>day_3_after</th>\n      <th>day_2_after</th>\n      <th>day_after</th>\n      <th>day_before</th>\n      <th>day_2_before</th>\n      <th>day_3_before</th>\n      <th>day_4_before</th>\n      <th>day_5_before</th>\n      <th>day_6_before</th>\n      <th>day_25_before</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>83545110</td>\n      <td>76568</td>\n      <td>10843</td>\n      <td>SONC</td>\n      <td>SONC</td>\n      <td>SONIC CORP</td>\n      <td>2010-01-05 16:00:00</td>\n      <td>-0.03368</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2010-01-08</td>\n      <td>2010-01-07</td>\n      <td>2010-01-06</td>\n      <td>2010-01-05</td>\n      <td>2010-01-04</td>\n      <td>2009-12-31</td>\n      <td>2009-12-30</td>\n      <td>2009-12-29</td>\n      <td>2009-12-28</td>\n      <td>2009-11-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03475V10</td>\n      <td>90179</td>\n      <td>45192</td>\n      <td>ANGO</td>\n      <td>ANGO</td>\n      <td>ANGIODYNAMICS</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>0.02000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2010-01-08</td>\n      <td>2010-01-07</td>\n      <td>2010-01-06</td>\n      <td>2010-01-05</td>\n      <td>2010-01-04</td>\n      <td>2009-12-31</td>\n      <td>2009-12-30</td>\n      <td>2009-12-29</td>\n      <td>2009-12-28</td>\n      <td>2009-11-30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51476610</td>\n      <td>83148</td>\n      <td>14374</td>\n      <td>LNDC</td>\n      <td>LNDC</td>\n      <td>LANDEC CORP</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>-0.00333</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2010-01-08</td>\n      <td>2010-01-07</td>\n      <td>2010-01-06</td>\n      <td>2010-01-05</td>\n      <td>2010-01-04</td>\n      <td>2009-12-31</td>\n      <td>2009-12-30</td>\n      <td>2009-12-29</td>\n      <td>2009-12-28</td>\n      <td>2009-11-30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>87815510</td>\n      <td>65752</td>\n      <td>4402</td>\n      <td>TISI</td>\n      <td>TISI</td>\n      <td>TEAM INC</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>0.04250</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2010-01-08</td>\n      <td>2010-01-07</td>\n      <td>2010-01-06</td>\n      <td>2010-01-05</td>\n      <td>2010-01-04</td>\n      <td>2009-12-31</td>\n      <td>2009-12-30</td>\n      <td>2009-12-29</td>\n      <td>2009-12-28</td>\n      <td>2009-11-30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>61945A10</td>\n      <td>90386</td>\n      <td>45695</td>\n      <td>MOS</td>\n      <td>MOS</td>\n      <td>MOSAIC CO/THE</td>\n      <td>2010-01-05 16:15:00</td>\n      <td>-0.03176</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2010-01-08</td>\n      <td>2010-01-07</td>\n      <td>2010-01-06</td>\n      <td>2010-01-05</td>\n      <td>2010-01-04</td>\n      <td>2009-12-31</td>\n      <td>2009-12-30</td>\n      <td>2009-12-29</td>\n      <td>2009-12-28</td>\n      <td>2009-11-30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>192411</th>\n      <td>69012T20</td>\n      <td>16104</td>\n      <td>55569</td>\n      <td>OTLK</td>\n      <td>OTLK</td>\n      <td>OUTLOOK</td>\n      <td>2023-12-22 08:05:00</td>\n      <td>0.00875</td>\n      <td>True</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2023-12-27</td>\n      <td>2023-12-26</td>\n      <td>2023-12-22</td>\n      <td>2023-12-21</td>\n      <td>2023-12-20</td>\n      <td>2023-12-19</td>\n      <td>2023-12-18</td>\n      <td>2023-12-15</td>\n      <td>2023-12-14</td>\n      <td>2023-11-16</td>\n    </tr>\n    <tr>\n      <th>192412</th>\n      <td>12482W30</td>\n      <td>17091</td>\n      <td>56139</td>\n      <td>YCBD</td>\n      <td>YCBD</td>\n      <td>CBDMD</td>\n      <td>2023-12-22 16:00:00</td>\n      <td>-0.45000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2023-12-28</td>\n      <td>2023-12-27</td>\n      <td>2023-12-26</td>\n      <td>2023-12-22</td>\n      <td>2023-12-21</td>\n      <td>2023-12-20</td>\n      <td>2023-12-19</td>\n      <td>2023-12-18</td>\n      <td>2023-12-15</td>\n      <td>2023-11-17</td>\n    </tr>\n    <tr>\n      <th>192413</th>\n      <td>16842Q10</td>\n      <td>16883</td>\n      <td>56045</td>\n      <td>CSSE</td>\n      <td>CSSE</td>\n      <td>CHICKEN SOUP</td>\n      <td>2023-12-22 17:00:00</td>\n      <td>-0.68600</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2023-12-28</td>\n      <td>2023-12-27</td>\n      <td>2023-12-26</td>\n      <td>2023-12-22</td>\n      <td>2023-12-21</td>\n      <td>2023-12-20</td>\n      <td>2023-12-19</td>\n      <td>2023-12-18</td>\n      <td>2023-12-15</td>\n      <td>2023-11-17</td>\n    </tr>\n    <tr>\n      <th>192414</th>\n      <td>L7756P10</td>\n      <td>21955</td>\n      <td>58741</td>\n      <td>PROC</td>\n      <td>PROC</td>\n      <td>PROCAPS GROUP</td>\n      <td>2023-12-26 17:25:00</td>\n      <td>0.03000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2023-12-29</td>\n      <td>2023-12-28</td>\n      <td>2023-12-27</td>\n      <td>2023-12-26</td>\n      <td>2023-12-22</td>\n      <td>2023-12-21</td>\n      <td>2023-12-20</td>\n      <td>2023-12-19</td>\n      <td>2023-12-18</td>\n      <td>2023-11-20</td>\n    </tr>\n    <tr>\n      <th>192415</th>\n      <td>10807Q70</td>\n      <td>92091</td>\n      <td>52371</td>\n      <td>BLIN</td>\n      <td>BLIN</td>\n      <td>BRIDGELINE DIG</td>\n      <td>2023-12-27 16:00:00</td>\n      <td>0.01333</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>2024-01-02</td>\n      <td>2023-12-29</td>\n      <td>2023-12-28</td>\n      <td>2023-12-27</td>\n      <td>2023-12-26</td>\n      <td>2023-12-22</td>\n      <td>2023-12-21</td>\n      <td>2023-12-20</td>\n      <td>2023-12-19</td>\n      <td>2023-11-21</td>\n    </tr>\n  </tbody>\n</table>\n<p>192416 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea_dates\n",
    "\n",
    "# Quick analysis\n",
    "# ea_dates.shape # 192416\n",
    "# ea_dates['pre_open'].sum() # 86646; 45.0% are pre-open\n",
    "# ea_dates['post_close'].sum() # 105770; 55.0% are post_close"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:22:26.196777300Z",
     "start_time": "2024-08-24T21:22:26.109827600Z"
    }
   },
   "id": "e91df62a68ab1490",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CRSP Data Cleaning + Merge EA Dates w/ CRSP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee0f2066175128fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get list of unique CUSIPs from IBES quarterly announcements \n",
    "# Used for downloading less data from WRDS\n",
    "\n",
    "\n",
    "eps = pd.read_csv(working_folder + '/statsum_unadjsurp_clean.csv', delimiter = ',')\n",
    "\n",
    "# Extract unique values from the 'CUSIP' column\n",
    "unique_cusip = eps['CUSIP'].unique()\n",
    "\n",
    "# Save the unique values to a .txt file\n",
    "with open('D:/FINRA_Working_Data/unique_cusip.txt', 'w') as file:\n",
    "    for cusip in unique_cusip:\n",
    "\n",
    "        file.write(f\"{cusip}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-16T15:27:12.214174400Z",
     "start_time": "2024-08-16T15:27:11.736911500Z"
    }
   },
   "id": "4d674917f909821e",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Clean the CRSP data downloaded from WRDS 8/15/24\n",
    "# CRSP Daily Stock file for 2009-2023, only for CUSIPs from cleaned IBES quarterly EAs\n",
    "\n",
    "\n",
    "# Specify the correct data types for the problematic columns\n",
    "dtype_spec = {\n",
    "    'PERMNO': 'str',\n",
    "    'SICCD': 'str',\n",
    "    'NCUSIP': 'str',\n",
    "    'SHRCLS': 'str',\n",
    "    'PERMCO': 'str',\n",
    "    'CUSIP': 'str',\n",
    "    'RET': 'str',\n",
    "    'RETX': 'str'\n",
    "}\n",
    "\n",
    "# Import the data with specified data types\n",
    "crsp = pd.read_csv(working_folder + '/crsp_daily_original.csv', delimiter = ',', dtype=dtype_spec, parse_dates=['date'], date_format='%Y-%m-%d')\n",
    "crsp['date'] = pd.to_datetime(crsp['date']).dt.tz_localize(None)\n",
    "\n",
    "# Fix column data types\n",
    "for col in ['SICCD', 'SHRCD', 'NAICS']: # to int\n",
    "    crsp[col] = pd.to_numeric(crsp[col], errors='coerce') # replace values that can't be converted with NaN\n",
    "    crsp = crsp.dropna(subset=[col])\n",
    "    crsp[col] = crsp[col].astype(int)\n",
    "\n",
    "for col in ['RET', 'RETX']: # to float\n",
    "    crsp[col] = pd.to_numeric(crsp[col], errors='coerce') # replace values that can't be converted with NaN\n",
    "    crsp = crsp.dropna(subset=[col])\n",
    "    crsp[col] = crsp[col].astype(float)\n",
    "\n",
    "# Adjust prices and number of shares\n",
    "crsp = crsp.dropna(subset=['CFACPR', 'CFACSHR'])\n",
    "crsp = crsp[crsp['CFACPR'] != 0]\n",
    "crsp['PRC'] = crsp['PRC'].abs()\n",
    "crsp['PRC_scaled'] = crsp['PRC']/crsp['CFACPR']\n",
    "crsp['SHROUT'] = crsp['SHROUT']*crsp['CFACSHR']\n",
    "\n",
    "# Drop thinly traded stocks (<$5) based on PRC, not PRC_scaled\n",
    "\n",
    "# Drop share class column and missing values of volume\n",
    "crsp = crsp.drop(columns=['SHRCLS'])\n",
    "crsp = crsp.dropna(subset=['VOL'])\n",
    "crsp = crsp[crsp['VOL']>0]\n",
    "\n",
    "crsp = crsp.drop(columns=['BID', 'ASK', 'BIDLO', 'ASKHI', 'NUMTRD'])\n",
    "\n",
    "crsp = crsp[~crsp['RET'].isin([-66, -77, -88, -99])].reset_index(drop=True)\n",
    "crsp = crsp.sort_values(by=['PERMNO', 'date'])\n",
    "crsp['LRET1'] = crsp.groupby('PERMNO')['RET'].shift(1)\n",
    "crsp['LRET2'] = crsp.groupby('PERMNO')['RET'].shift(2)\n",
    "crsp['LRET3'] = crsp.groupby('PERMNO')['RET'].shift(3)\n",
    "crsp['LRET4'] = crsp.groupby('PERMNO')['RET'].shift(4)\n",
    "crsp['LRET5'] = crsp.groupby('PERMNO')['RET'].shift(5)\n",
    "crsp['FRET1'] = crsp.groupby('PERMNO')['RET'].shift(-1)\n",
    "crsp['FRET2'] = crsp.groupby('PERMNO')['RET'].shift(-2)\n",
    "crsp['FRET3'] = crsp.groupby('PERMNO')['RET'].shift(-3)\n",
    "crsp['FRET4'] = crsp.groupby('PERMNO')['RET'].shift(-4)\n",
    "crsp['FRET5'] = crsp.groupby('PERMNO')['RET'].shift(-5)\n",
    "crsp['RET1_2'] = crsp['RET'] + crsp['FRET1']\n",
    "crsp['RET1_3'] = crsp['RET'] + crsp['FRET1'] + crsp['FRET2']\n",
    "\n",
    "crsp = crsp.dropna()\n",
    "crsp.to_csv(working_folder + '/crsp_daily_clean.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-23T21:10:56.461912Z",
     "start_time": "2024-08-23T21:05:14.053699700Z"
    }
   },
   "id": "b8ba6d35a1005d96",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   PERMNO       date  SHRCD  SICCD    NCUSIP TICKER           COMNAM   NAICS  \\\n0   10001 2009-01-09     11   4920  29274A20   EWST  ENERGY WEST INC  221210   \n1   10001 2009-01-12     11   4920  29274A20   EWST  ENERGY WEST INC  221210   \n2   10001 2009-01-13     11   4920  29274A20   EWST  ENERGY WEST INC  221210   \n3   10001 2009-01-14     11   4920  29274A20   EWST  ENERGY WEST INC  221210   \n4   10001 2009-01-15     11   4920  29274A20   EWST  ENERGY WEST INC  221210   \n\n   PERMCO     CUSIP  ...     LRET3     LRET4     LRET5     FRET1     FRET2  \\\n0    7953  36720410  ...  0.012983  0.013301  0.001211 -0.017321  0.009389   \n1    7953  36720410  ... -0.009283  0.012983  0.013301  0.009389 -0.003497   \n2    7953  36720410  ... -0.007134 -0.009283  0.012983 -0.003497  0.038732   \n3    7953  36720410  ...  0.037126 -0.007134 -0.009283  0.038732 -0.039548   \n4    7953  36720410  ... -0.017321  0.037126 -0.007134 -0.039548  0.005294   \n\n      FRET3     FRET4     FRET5    RET1_2    RET1_3  \n0 -0.003497  0.038732 -0.039548  0.019805  0.029194  \n1  0.038732 -0.039548  0.005294 -0.007932 -0.011429  \n2 -0.039548  0.005294 -0.020948  0.005892  0.044624  \n3  0.005294 -0.020948  0.074576  0.035235 -0.004313  \n4 -0.020948  0.074576 -0.054494 -0.000816  0.004478  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>date</th>\n      <th>SHRCD</th>\n      <th>SICCD</th>\n      <th>NCUSIP</th>\n      <th>TICKER</th>\n      <th>COMNAM</th>\n      <th>NAICS</th>\n      <th>PERMCO</th>\n      <th>CUSIP</th>\n      <th>...</th>\n      <th>LRET3</th>\n      <th>LRET4</th>\n      <th>LRET5</th>\n      <th>FRET1</th>\n      <th>FRET2</th>\n      <th>FRET3</th>\n      <th>FRET4</th>\n      <th>FRET5</th>\n      <th>RET1_2</th>\n      <th>RET1_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10001</td>\n      <td>2009-01-09</td>\n      <td>11</td>\n      <td>4920</td>\n      <td>29274A20</td>\n      <td>EWST</td>\n      <td>ENERGY WEST INC</td>\n      <td>221210</td>\n      <td>7953</td>\n      <td>36720410</td>\n      <td>...</td>\n      <td>0.012983</td>\n      <td>0.013301</td>\n      <td>0.001211</td>\n      <td>-0.017321</td>\n      <td>0.009389</td>\n      <td>-0.003497</td>\n      <td>0.038732</td>\n      <td>-0.039548</td>\n      <td>0.019805</td>\n      <td>0.029194</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10001</td>\n      <td>2009-01-12</td>\n      <td>11</td>\n      <td>4920</td>\n      <td>29274A20</td>\n      <td>EWST</td>\n      <td>ENERGY WEST INC</td>\n      <td>221210</td>\n      <td>7953</td>\n      <td>36720410</td>\n      <td>...</td>\n      <td>-0.009283</td>\n      <td>0.012983</td>\n      <td>0.013301</td>\n      <td>0.009389</td>\n      <td>-0.003497</td>\n      <td>0.038732</td>\n      <td>-0.039548</td>\n      <td>0.005294</td>\n      <td>-0.007932</td>\n      <td>-0.011429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10001</td>\n      <td>2009-01-13</td>\n      <td>11</td>\n      <td>4920</td>\n      <td>29274A20</td>\n      <td>EWST</td>\n      <td>ENERGY WEST INC</td>\n      <td>221210</td>\n      <td>7953</td>\n      <td>36720410</td>\n      <td>...</td>\n      <td>-0.007134</td>\n      <td>-0.009283</td>\n      <td>0.012983</td>\n      <td>-0.003497</td>\n      <td>0.038732</td>\n      <td>-0.039548</td>\n      <td>0.005294</td>\n      <td>-0.020948</td>\n      <td>0.005892</td>\n      <td>0.044624</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10001</td>\n      <td>2009-01-14</td>\n      <td>11</td>\n      <td>4920</td>\n      <td>29274A20</td>\n      <td>EWST</td>\n      <td>ENERGY WEST INC</td>\n      <td>221210</td>\n      <td>7953</td>\n      <td>36720410</td>\n      <td>...</td>\n      <td>0.037126</td>\n      <td>-0.007134</td>\n      <td>-0.009283</td>\n      <td>0.038732</td>\n      <td>-0.039548</td>\n      <td>0.005294</td>\n      <td>-0.020948</td>\n      <td>0.074576</td>\n      <td>0.035235</td>\n      <td>-0.004313</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10001</td>\n      <td>2009-01-15</td>\n      <td>11</td>\n      <td>4920</td>\n      <td>29274A20</td>\n      <td>EWST</td>\n      <td>ENERGY WEST INC</td>\n      <td>221210</td>\n      <td>7953</td>\n      <td>36720410</td>\n      <td>...</td>\n      <td>-0.017321</td>\n      <td>0.037126</td>\n      <td>-0.007134</td>\n      <td>-0.039548</td>\n      <td>0.005294</td>\n      <td>-0.020948</td>\n      <td>0.074576</td>\n      <td>-0.054494</td>\n      <td>-0.000816</td>\n      <td>0.004478</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crsp.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:33:14.776157400Z",
     "start_time": "2024-08-24T21:33:14.682752100Z"
    }
   },
   "id": "77a6d10390ba0ca0",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f96dcd15b61e48babeb384fec83b1496"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge IBES quarterly announcements with CRSP (on IBES CUSIP and CRSP NCUSIP)\n",
    "\n",
    "\n",
    "eps = pd.read_csv(working_folder + '/ea_dates.csv', delimiter = ',', parse_dates=['ANNOUNCE', 'day_after', 'day_2_after', 'day_3_after', 'day_4_after', 'day_5_after', 'day_before', 'day_2_before', 'day_3_before', 'day_4_before', 'day_5_before', 'day_6_before', 'day_25_before'], date_format='ISO8601')\n",
    "crsp = pd.read_csv(working_folder + '/crsp_daily_clean.csv', delimiter = ',', parse_dates=['date'], date_format='%Y-%m-%d', low_memory=False)\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('eps', eps)\n",
    "con.register('crsp', crsp)\n",
    "\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    eps.*, \n",
    "    crsp1.PRC_scaled AS price_6day_lag,\n",
    "    crsp2.CFACPR AS adjust_day_after,\n",
    "    crsp2.LRET5 as lret5,\n",
    "    crsp2.LRET4 as lret4,\n",
    "    crsp2.LRET3 as lret3,\n",
    "    crsp2.LRET2 as lret2,\n",
    "    crsp2.LRET1 as lret1,\n",
    "    crsp2.RET as ret,\n",
    "    crsp2.FRET1 as fret1,\n",
    "    crsp2.FRET2 as fret2,\n",
    "    crsp2.FRET3 as fret3,\n",
    "    crsp2.FRET4 as fret4,\n",
    "    crsp2.RET1_2 as ret1_2,\n",
    "    crsp2.RET1_3 as ret1_3,\n",
    "    COALESCE(crsp3.VOL,0) as day5_after_volume_crsp,\n",
    "    COALESCE(crsp4.VOL,0) as day4_after_volume_crsp,\n",
    "    COALESCE(crsp5.VOL,0) as day3_after_volume_crsp,\n",
    "    COALESCE(crsp6.VOL,0) as day2_before_volume_crsp,\n",
    "    COALESCE(crsp7.VOL,0) as day_before_volume_crsp,\n",
    "    COALESCE(crsp2.VOL,0) as day_after_volume_crsp,\n",
    "    COALESCE(crsp8.VOL,0) as day2_after_volume_crsp,\n",
    "    COALESCE(crsp9.VOL,0) as day3_before_volume_crsp,\n",
    "    COALESCE(crsp10.VOL,0) as day4_before_volume_crsp,\n",
    "    COALESCE(crsp11.VOL,0) as day5_before_volume_crsp,\n",
    "    COALESCE(crsp12.VOL,0) as day6_before_volume_crsp\n",
    "FROM \n",
    "    eps\n",
    "LEFT JOIN \n",
    "    crsp crsp1\n",
    "ON \n",
    "    eps.CUSIP = crsp1.NCUSIP \n",
    "    AND eps.day_6_before = crsp1.date\n",
    "LEFT JOIN \n",
    "    crsp crsp2\n",
    "ON \n",
    "    eps.CUSIP = crsp2.NCUSIP \n",
    "    AND eps.day_after = crsp2.date\n",
    "LEFT JOIN \n",
    "    crsp crsp3\n",
    "ON \n",
    "    eps.CUSIP = crsp3.NCUSIP \n",
    "    AND eps.day_5_after = crsp3.date\n",
    "LEFT JOIN \n",
    "    crsp crsp4\n",
    "ON \n",
    "    eps.CUSIP = crsp4.NCUSIP \n",
    "    AND eps.day_4_after = crsp4.date\n",
    "LEFT JOIN \n",
    "    crsp crsp5\n",
    "ON \n",
    "    eps.CUSIP = crsp5.NCUSIP \n",
    "    AND eps.day_3_after = crsp5.date\n",
    "LEFT JOIN \n",
    "    crsp crsp6\n",
    "ON \n",
    "    eps.CUSIP = crsp6.NCUSIP \n",
    "    AND eps.day_2_before = crsp6.date\n",
    "LEFT JOIN \n",
    "    crsp crsp7\n",
    "ON \n",
    "    eps.CUSIP = crsp7.NCUSIP \n",
    "    AND eps.day_before = crsp7.date\n",
    "LEFT JOIN \n",
    "    crsp crsp8\n",
    "ON \n",
    "    eps.CUSIP = crsp8.NCUSIP \n",
    "    AND eps.day_2_after = crsp8.date\n",
    "LEFT JOIN \n",
    "    crsp crsp9\n",
    "ON \n",
    "    eps.CUSIP = crsp9.NCUSIP \n",
    "    AND eps.day_3_before = crsp9.date\n",
    "LEFT JOIN \n",
    "    crsp crsp10\n",
    "ON \n",
    "    eps.CUSIP = crsp10.NCUSIP \n",
    "    AND eps.day_4_before = crsp10.date\n",
    "LEFT JOIN \n",
    "    crsp crsp11\n",
    "ON \n",
    "    eps.CUSIP = crsp11.NCUSIP \n",
    "    AND eps.day_5_before = crsp11.date\n",
    "LEFT JOIN \n",
    "    crsp crsp12\n",
    "ON \n",
    "    eps.CUSIP = crsp12.NCUSIP \n",
    "    AND eps.day_6_before = crsp12.date\n",
    "WHERE \n",
    "    crsp1.PRC_scaled IS NOT NULL \n",
    "    AND crsp2.CFACPR IS NOT NULL\n",
    "    AND (crsp7.VOL IS NOT NULL\n",
    "    OR crsp2.VOL IS NOT NULL)\n",
    "ORDER BY \n",
    "    eps.ANNOUNCE, eps.pit_ticker\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "eps_merge = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "con.close()\n",
    "\n",
    "# Scale earnings surprise by closing price one week before the announcement\n",
    "# Surprise and price are scaled by the corresponding day's adjustment factor\n",
    "eps_merge['SURPRISE'] = eps_merge['SURPRISE']/eps_merge['adjust_day_after']\n",
    "eps_merge['SURPRISE'] = eps_merge['SURPRISE']/eps_merge['price_6day_lag']\n",
    "\n",
    "\n",
    "eps_merge.to_csv(working_folder + '/ibes_crsp_merge.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:23:36.054836200Z",
     "start_time": "2024-08-24T21:22:26.195797500Z"
    }
   },
   "id": "1541f0ff854225d9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      CUSIP  permno  permco pit_ticker OFTIC          CNAME  \\\n0  83545110   76568   10843       SONC  SONC     SONIC CORP   \n1  03475V10   90179   45192       ANGO  ANGO  ANGIODYNAMICS   \n2  51476610   83148   14374       LNDC  LNDC    LANDEC CORP   \n3  87815510   65752    4402       TISI  TISI       TEAM INC   \n4  61945A10   90386   45695        MOS   MOS  MOSAIC CO/THE   \n\n             ANNOUNCE  SURPRISE  POS_SURPRISE  pre_open  ...  \\\n0 2010-01-05 16:00:00 -0.003198         False     False  ...   \n1 2010-01-05 16:05:00  0.001271          True     False  ...   \n2 2010-01-05 16:05:00 -0.000556         False     False  ...   \n3 2010-01-05 16:05:00  0.002276          True     False  ...   \n4 2010-01-05 16:15:00 -0.000522         False     False  ...   \n\n   day4_after_volume_crsp day3_after_volume_crsp day2_before_volume_crsp  \\\n0               1355876.0              1411107.0                759771.0   \n1                144964.0               322411.0                322350.0   \n2                 39801.0                39503.0                 71586.0   \n3                102107.0                53627.0                 53261.0   \n4               4031600.0              4074700.0               4072600.0   \n\n  day_before_volume_crsp day_after_volume_crsp day2_after_volume_crsp  \\\n0               888134.0             6754285.0              2032954.0   \n1               157386.0              975932.0               281087.0   \n2                64962.0              180997.0               135191.0   \n3               117312.0              106846.0               143918.0   \n4              6505800.0             9999900.0              5544000.0   \n\n  day3_before_volume_crsp day4_before_volume_crsp day5_before_volume_crsp  \\\n0                864570.0                437752.0                215709.0   \n1                 63337.0                 81643.0                 62245.0   \n2                102517.0                107020.0                205939.0   \n3                 53621.0                 63789.0                 73291.0   \n4               1821200.0               1871500.0               3227500.0   \n\n  day6_before_volume_crsp  \n0                248900.0  \n1                 62007.0  \n2                 77058.0  \n3                 48777.0  \n4               2311700.0  \n\n[5 rows x 48 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSIP</th>\n      <th>permno</th>\n      <th>permco</th>\n      <th>pit_ticker</th>\n      <th>OFTIC</th>\n      <th>CNAME</th>\n      <th>ANNOUNCE</th>\n      <th>SURPRISE</th>\n      <th>POS_SURPRISE</th>\n      <th>pre_open</th>\n      <th>...</th>\n      <th>day4_after_volume_crsp</th>\n      <th>day3_after_volume_crsp</th>\n      <th>day2_before_volume_crsp</th>\n      <th>day_before_volume_crsp</th>\n      <th>day_after_volume_crsp</th>\n      <th>day2_after_volume_crsp</th>\n      <th>day3_before_volume_crsp</th>\n      <th>day4_before_volume_crsp</th>\n      <th>day5_before_volume_crsp</th>\n      <th>day6_before_volume_crsp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>83545110</td>\n      <td>76568</td>\n      <td>10843</td>\n      <td>SONC</td>\n      <td>SONC</td>\n      <td>SONIC CORP</td>\n      <td>2010-01-05 16:00:00</td>\n      <td>-0.003198</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1355876.0</td>\n      <td>1411107.0</td>\n      <td>759771.0</td>\n      <td>888134.0</td>\n      <td>6754285.0</td>\n      <td>2032954.0</td>\n      <td>864570.0</td>\n      <td>437752.0</td>\n      <td>215709.0</td>\n      <td>248900.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03475V10</td>\n      <td>90179</td>\n      <td>45192</td>\n      <td>ANGO</td>\n      <td>ANGO</td>\n      <td>ANGIODYNAMICS</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>0.001271</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>144964.0</td>\n      <td>322411.0</td>\n      <td>322350.0</td>\n      <td>157386.0</td>\n      <td>975932.0</td>\n      <td>281087.0</td>\n      <td>63337.0</td>\n      <td>81643.0</td>\n      <td>62245.0</td>\n      <td>62007.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51476610</td>\n      <td>83148</td>\n      <td>14374</td>\n      <td>LNDC</td>\n      <td>LNDC</td>\n      <td>LANDEC CORP</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>-0.000556</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>39801.0</td>\n      <td>39503.0</td>\n      <td>71586.0</td>\n      <td>64962.0</td>\n      <td>180997.0</td>\n      <td>135191.0</td>\n      <td>102517.0</td>\n      <td>107020.0</td>\n      <td>205939.0</td>\n      <td>77058.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>87815510</td>\n      <td>65752</td>\n      <td>4402</td>\n      <td>TISI</td>\n      <td>TISI</td>\n      <td>TEAM INC</td>\n      <td>2010-01-05 16:05:00</td>\n      <td>0.002276</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>102107.0</td>\n      <td>53627.0</td>\n      <td>53261.0</td>\n      <td>117312.0</td>\n      <td>106846.0</td>\n      <td>143918.0</td>\n      <td>53621.0</td>\n      <td>63789.0</td>\n      <td>73291.0</td>\n      <td>48777.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>61945A10</td>\n      <td>90386</td>\n      <td>45695</td>\n      <td>MOS</td>\n      <td>MOS</td>\n      <td>MOSAIC CO/THE</td>\n      <td>2010-01-05 16:15:00</td>\n      <td>-0.000522</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>4031600.0</td>\n      <td>4074700.0</td>\n      <td>4072600.0</td>\n      <td>6505800.0</td>\n      <td>9999900.0</td>\n      <td>5544000.0</td>\n      <td>1821200.0</td>\n      <td>1871500.0</td>\n      <td>3227500.0</td>\n      <td>2311700.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 48 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_merge.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:33:05.136951600Z",
     "start_time": "2024-08-24T21:33:04.967614600Z"
    }
   },
   "id": "1474ba3bde3ed50f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          CUSIP  permno  permco pit_ticker OFTIC         CNAME  \\\n66975  00030710   14945   55038        AAC   AAC  AAC HOLDINGS   \n69313  00030710   14945   55038        AAC   AAC  AAC HOLDINGS   \n72754  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n75894  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n80460  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n83871  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n87205  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n90392  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n94219  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n96921  00030710   14945   55038        AAC   AAC     AAC HOLDG   \n\n                 ANNOUNCE  SURPRISE  POS_SURPRISE  pre_open  ...  \\\n66975 2015-02-24 16:05:00  0.003519          True     False  ...   \n69313 2015-04-29 16:05:00  0.001504          True     False  ...   \n72754 2015-07-29 16:50:00  0.004313          True     False  ...   \n75894 2015-10-27 17:15:00  0.002426          True     False  ...   \n80460 2016-02-23 06:30:00 -0.000298         False      True  ...   \n83871 2016-05-05 06:30:00  0.002542          True      True  ...   \n87205 2016-08-04 06:30:00 -0.001913         False      True  ...   \n90392 2016-11-03 06:30:00 -0.002134         False      True  ...   \n94219 2017-02-27 18:50:00 -0.000617         False     False  ...   \n96921 2017-05-03 18:35:00  0.009346          True     False  ...   \n\n       day_before_volume_crsp day_after_volume_crsp day2_after_volume_crsp  \\\n66975                159273.0              174032.0               150677.0   \n69313                205740.0              721994.0               182544.0   \n72754                214241.0             1065299.0               525575.0   \n75894                264274.0              659659.0               269322.0   \n80460                378930.0              572062.0               185404.0   \n83871                112262.0              464244.0               148874.0   \n87205                232277.0              304968.0               270909.0   \n90392                246999.0             3742602.0              1246354.0   \n94219                173443.0              418117.0               307752.0   \n96921                104717.0              341148.0               273381.0   \n\n      day3_before_volume_crsp day4_before_volume_crsp day5_before_volume_crsp  \\\n66975                 94039.0                  9343.0                 35211.0   \n69313                103761.0                 99156.0                190891.0   \n72754                372503.0                256674.0                253017.0   \n75894                122660.0                410267.0                204418.0   \n80460                239184.0                320303.0                211663.0   \n83871                152641.0                137904.0                210377.0   \n87205                181192.0                 67323.0                117135.0   \n90392                153957.0                 95531.0                 87061.0   \n94219                165743.0                 87871.0                 84583.0   \n96921                 51342.0                 58351.0                 72729.0   \n\n      day6_before_volume_crsp  year quarter year_quarter  \n66975                 18925.0  2015       1           21  \n69313                 71138.0  2015       2           22  \n72754                330927.0  2015       3           23  \n75894                193906.0  2015       4           24  \n80460                271571.0  2016       1           25  \n83871                 88539.0  2016       2           26  \n87205                185273.0  2016       3           27  \n90392                161290.0  2016       4           28  \n94219                 84704.0  2017       1           29  \n96921                121080.0  2017       2           30  \n\n[10 rows x 51 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSIP</th>\n      <th>permno</th>\n      <th>permco</th>\n      <th>pit_ticker</th>\n      <th>OFTIC</th>\n      <th>CNAME</th>\n      <th>ANNOUNCE</th>\n      <th>SURPRISE</th>\n      <th>POS_SURPRISE</th>\n      <th>pre_open</th>\n      <th>...</th>\n      <th>day_before_volume_crsp</th>\n      <th>day_after_volume_crsp</th>\n      <th>day2_after_volume_crsp</th>\n      <th>day3_before_volume_crsp</th>\n      <th>day4_before_volume_crsp</th>\n      <th>day5_before_volume_crsp</th>\n      <th>day6_before_volume_crsp</th>\n      <th>year</th>\n      <th>quarter</th>\n      <th>year_quarter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>66975</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDINGS</td>\n      <td>2015-02-24 16:05:00</td>\n      <td>0.003519</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>159273.0</td>\n      <td>174032.0</td>\n      <td>150677.0</td>\n      <td>94039.0</td>\n      <td>9343.0</td>\n      <td>35211.0</td>\n      <td>18925.0</td>\n      <td>2015</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>69313</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDINGS</td>\n      <td>2015-04-29 16:05:00</td>\n      <td>0.001504</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>205740.0</td>\n      <td>721994.0</td>\n      <td>182544.0</td>\n      <td>103761.0</td>\n      <td>99156.0</td>\n      <td>190891.0</td>\n      <td>71138.0</td>\n      <td>2015</td>\n      <td>2</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>72754</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2015-07-29 16:50:00</td>\n      <td>0.004313</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>214241.0</td>\n      <td>1065299.0</td>\n      <td>525575.0</td>\n      <td>372503.0</td>\n      <td>256674.0</td>\n      <td>253017.0</td>\n      <td>330927.0</td>\n      <td>2015</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>75894</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2015-10-27 17:15:00</td>\n      <td>0.002426</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>264274.0</td>\n      <td>659659.0</td>\n      <td>269322.0</td>\n      <td>122660.0</td>\n      <td>410267.0</td>\n      <td>204418.0</td>\n      <td>193906.0</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>80460</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-02-23 06:30:00</td>\n      <td>-0.000298</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>378930.0</td>\n      <td>572062.0</td>\n      <td>185404.0</td>\n      <td>239184.0</td>\n      <td>320303.0</td>\n      <td>211663.0</td>\n      <td>271571.0</td>\n      <td>2016</td>\n      <td>1</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>83871</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-05-05 06:30:00</td>\n      <td>0.002542</td>\n      <td>True</td>\n      <td>True</td>\n      <td>...</td>\n      <td>112262.0</td>\n      <td>464244.0</td>\n      <td>148874.0</td>\n      <td>152641.0</td>\n      <td>137904.0</td>\n      <td>210377.0</td>\n      <td>88539.0</td>\n      <td>2016</td>\n      <td>2</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>87205</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-08-04 06:30:00</td>\n      <td>-0.001913</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>232277.0</td>\n      <td>304968.0</td>\n      <td>270909.0</td>\n      <td>181192.0</td>\n      <td>67323.0</td>\n      <td>117135.0</td>\n      <td>185273.0</td>\n      <td>2016</td>\n      <td>3</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>90392</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-11-03 06:30:00</td>\n      <td>-0.002134</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>246999.0</td>\n      <td>3742602.0</td>\n      <td>1246354.0</td>\n      <td>153957.0</td>\n      <td>95531.0</td>\n      <td>87061.0</td>\n      <td>161290.0</td>\n      <td>2016</td>\n      <td>4</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>94219</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2017-02-27 18:50:00</td>\n      <td>-0.000617</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>173443.0</td>\n      <td>418117.0</td>\n      <td>307752.0</td>\n      <td>165743.0</td>\n      <td>87871.0</td>\n      <td>84583.0</td>\n      <td>84704.0</td>\n      <td>2017</td>\n      <td>1</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>96921</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2017-05-03 18:35:00</td>\n      <td>0.009346</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>104717.0</td>\n      <td>341148.0</td>\n      <td>273381.0</td>\n      <td>51342.0</td>\n      <td>58351.0</td>\n      <td>72729.0</td>\n      <td>121080.0</td>\n      <td>2017</td>\n      <td>2</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 51 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep IBES/CRSP merged data to check ERCs\n",
    "\n",
    "\n",
    "reg_test = pd.read_csv(working_folder + '/ibes_crsp_merge.csv', delimiter = ',', parse_dates=['ANNOUNCE'], date_format='ISO8601')\n",
    "\n",
    "reg_test.loc[:, 'year'] = reg_test['ANNOUNCE'].dt.year\n",
    "reg_test.loc[:, 'quarter'] = reg_test['ANNOUNCE'].dt.quarter\n",
    "reg_test.loc[:,'year_quarter'] = (reg_test['year'] - 2010) * 4 + reg_test['quarter']\n",
    "\n",
    "# Drop duplicates, keeping the later EA\n",
    "reg_test = reg_test.sort_values(by=['CUSIP', 'year_quarter', 'ANNOUNCE'])\n",
    "reg_test = reg_test.drop_duplicates(subset=['CUSIP', 'year_quarter'], keep='last')\n",
    "\n",
    "reg_test.to_stata(working_folder + '/reg_test.dta')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:23:37.913600300Z",
     "start_time": "2024-08-24T21:23:36.095009200Z"
    }
   },
   "id": "1679e20eedc22d8e",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge EAs with Shorts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a5b188775fd82c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Gather shorting volume around EAs \n",
    "\n",
    "\n",
    "reg_data = pd.read_csv(working_folder + '/ibes_crsp_merge.csv', delimiter = ',', parse_dates=['ANNOUNCE', 'day_after', 'day_before', 'day_2_before', 'day_3_before', 'day_4_before', 'day_5_before', 'day_6_before', 'day_25_before'], date_format='ISO8601')\n",
    "\n",
    "for year in range(2010, 2024):\n",
    "    file_path = f'D:/FINRA_Daily_Agg/agg_{year}.pkl'\n",
    "\n",
    "    # Read the data into a dataframe with the same name as the file\n",
    "    globals()[f'agg_{year}'] = pd.read_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:41:35.723297700Z",
     "start_time": "2024-08-24T21:41:32.476270200Z"
    }
   },
   "id": "550de68b592a8735",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53386521e4b64df4882bfe20877eebb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Register the DataFrame with the DuckDB instance\n",
    "con.register('reg_data', reg_data)\n",
    "\n",
    "# Merge reg_data with agg_ files\n",
    "query = \"\"\"\n",
    "WITH short_data AS (\n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2010 UNION ALL\n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2011 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2012 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2013 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2014 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2015 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2016 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2017 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2018 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2019 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2020 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2021 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2022 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2023\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    reg_data.*, \n",
    "    COALESCE(short_data_day5_before.Shorts, 0) AS day5_be_shorts,\n",
    "    COALESCE(short_data_day4_before.Shorts, 0) AS day4_be_shorts,\n",
    "    COALESCE(short_data_day3_before.Shorts, 0) AS day3_be_shorts,\n",
    "    COALESCE(short_data_day2_before.Shorts, 0) AS day2_be_shorts,\n",
    "    COALESCE(short_data_day_before.Shorts, 0) AS day_be_shorts,\n",
    "    COALESCE(short_data_day_after.Shorts, 0) AS day_af_shorts,\n",
    "    COALESCE(short_data_day2_after.Shorts, 0) AS day2_af_shorts,\n",
    "    COALESCE(short_data_day3_after.Shorts, 0) AS day3_af_shorts,\n",
    "    COALESCE(short_data_day4_after.Shorts, 0) AS day4_af_shorts,\n",
    "    COALESCE(short_data_day5_after.Shorts, 0) AS day5_af_shorts\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN short_data AS short_data_day5_before\n",
    "    ON reg_data.pit_ticker = short_data_day5_before.Symbol \n",
    "    AND reg_data.day_5_before = short_data_day5_before.Date\n",
    "LEFT JOIN short_data AS short_data_day4_before\n",
    "    ON reg_data.pit_ticker = short_data_day4_before.Symbol \n",
    "    AND reg_data.day_4_before = short_data_day4_before.Date\n",
    "LEFT JOIN short_data AS short_data_day3_before\n",
    "    ON reg_data.pit_ticker = short_data_day3_before.Symbol \n",
    "    AND reg_data.day_3_before = short_data_day3_before.Date\n",
    "LEFT JOIN short_data AS short_data_day2_before\n",
    "    ON reg_data.pit_ticker = short_data_day2_before.Symbol \n",
    "    AND reg_data.day_2_before = short_data_day2_before.Date\n",
    "LEFT JOIN short_data AS short_data_day_before\n",
    "    ON reg_data.pit_ticker = short_data_day_before.Symbol \n",
    "    AND reg_data.day_before = short_data_day_before.Date\n",
    "LEFT JOIN short_data AS short_data_day_after\n",
    "    ON reg_data.pit_ticker = short_data_day_after.Symbol \n",
    "    AND reg_data.day_after = short_data_day_after.Date\n",
    "LEFT JOIN short_data AS short_data_day2_after\n",
    "    ON reg_data.pit_ticker = short_data_day2_after.Symbol \n",
    "    AND reg_data.day_2_after = short_data_day2_after.Date\n",
    "LEFT JOIN short_data AS short_data_day3_after\n",
    "    ON reg_data.pit_ticker = short_data_day3_after.Symbol \n",
    "    AND reg_data.day_3_after = short_data_day3_after.Date\n",
    "LEFT JOIN short_data AS short_data_day4_after\n",
    "    ON reg_data.pit_ticker = short_data_day4_after.Symbol \n",
    "    AND reg_data.day_4_after = short_data_day4_after.Date\n",
    "LEFT JOIN short_data AS short_data_day5_after\n",
    "    ON reg_data.pit_ticker = short_data_day5_after.Symbol \n",
    "    AND reg_data.day_5_after = short_data_day5_after.Date\n",
    "WHERE \n",
    "    short_data_day_before.Shorts IS NOT NULL \n",
    "    OR short_data_day_after.Shorts IS NOT NULL\n",
    "ORDER BY \n",
    "    reg_data.ANNOUNCE, \n",
    "    reg_data.pit_ticker\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:41:59.930585500Z",
     "start_time": "2024-08-24T21:41:35.716233300Z"
    }
   },
   "id": "5fc04c8491acd0e9",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Average shorting one month prior\n",
    "\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('reg_data', reg_data)\n",
    "\n",
    "# Merge reg_data with agg_ files\n",
    "query = \"\"\"\n",
    "WITH short_data AS (\n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2010 UNION ALL\n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2011 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2012 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2013 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2014 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2015 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2016 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2017 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2018 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2019 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2020 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2021 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2022 UNION ALL \n",
    "    SELECT Symbol, Date, Shorts\n",
    "    FROM agg_2023\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    reg_data.pit_ticker, reg_data.ANNOUNCE,\n",
    "    SUM(COALESCE(short_data.Shorts, 0)) / 20 AS avg_month_before_shorts\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN short_data\n",
    "    ON reg_data.pit_ticker = short_data.Symbol \n",
    "    AND short_data.Date >= reg_data.day_25_before \n",
    "    AND short_data.Date <= reg_data.day_6_before\n",
    "GROUP BY\n",
    "    reg_data.pit_ticker,\n",
    "    reg_data.ANNOUNCE\n",
    "ORDER BY \n",
    "    reg_data.pit_ticker,\n",
    "    reg_data.ANNOUNCE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data_avg_short = con.execute(query).df()\n",
    "\n",
    "# Merge reg_data with agg_ files\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    reg_data.*, reg_data_avg_short.avg_month_before_shorts\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN reg_data_avg_short\n",
    "    ON reg_data.pit_ticker = reg_data_avg_short.pit_ticker \n",
    "    AND reg_data.ANNOUNCE = reg_data_avg_short.ANNOUNCE \n",
    "WHERE \n",
    "    reg_data_avg_short.avg_month_before_shorts IS NOT NULL\n",
    "ORDER BY \n",
    "    reg_data.pit_ticker,\n",
    "    reg_data.ANNOUNCE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:42:01.863324600Z",
     "start_time": "2024-08-24T21:41:59.940172800Z"
    }
   },
   "id": "8125aa0be1c2adeb",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99f1565cf4e44d189f58bb9001fc717a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "volume = pd.read_pickle('D:/FINRA_Volume/daily_volume.pkl')\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('reg_data', reg_data)\n",
    "con.register('volume', volume)\n",
    "\n",
    "# Merge volume with regression data\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    reg_data.*,\n",
    "    COALESCE(volume_day6_before.TotalVolume, 0) AS day6_before_volume,\n",
    "    COALESCE(volume_day5_before.TotalVolume, 0) AS day5_before_volume,\n",
    "    COALESCE(volume_day4_before.TotalVolume, 0) AS day4_before_volume,\n",
    "    COALESCE(volume_day3_before.TotalVolume, 0) AS day3_before_volume,\n",
    "    COALESCE(volume_day2_before.TotalVolume, 0) AS day2_before_volume,\n",
    "    volume_day_before.TotalVolume AS day_before_volume,\n",
    "    volume_day_after.TotalVolume AS day_after_volume,\n",
    "    COALESCE(volume_day2_after.TotalVolume, 0) AS day2_after_volume,\n",
    "    COALESCE(volume_day3_after.TotalVolume, 0) AS day3_after_volume,\n",
    "    COALESCE(volume_day4_after.TotalVolume, 0) AS day4_after_volume,\n",
    "    COALESCE(volume_day5_after.TotalVolume, 0) AS day5_after_volume\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN volume AS volume_day6_before\n",
    "    ON reg_data.pit_ticker = volume_day6_before.Symbol \n",
    "    AND reg_data.day_6_before = volume_day6_before.Date\n",
    "LEFT JOIN volume AS volume_day5_before\n",
    "    ON reg_data.pit_ticker = volume_day5_before.Symbol \n",
    "    AND reg_data.day_5_before = volume_day5_before.Date\n",
    "LEFT JOIN volume AS volume_day4_before\n",
    "    ON reg_data.pit_ticker = volume_day4_before.Symbol \n",
    "    AND reg_data.day_4_before = volume_day4_before.Date\n",
    "LEFT JOIN volume AS volume_day3_before\n",
    "    ON reg_data.pit_ticker = volume_day3_before.Symbol \n",
    "    AND reg_data.day_3_before = volume_day3_before.Date\n",
    "LEFT JOIN volume AS volume_day2_before\n",
    "    ON reg_data.pit_ticker = volume_day2_before.Symbol \n",
    "    AND reg_data.day_2_before = volume_day2_before.Date\n",
    "LEFT JOIN volume AS volume_day_before\n",
    "    ON reg_data.pit_ticker = volume_day_before.Symbol \n",
    "    AND reg_data.day_before = volume_day_before.Date\n",
    "LEFT JOIN volume AS volume_day_after\n",
    "    ON reg_data.pit_ticker = volume_day_after.Symbol \n",
    "    AND reg_data.day_after = volume_day_after.Date\n",
    "LEFT JOIN volume AS volume_day2_after\n",
    "    ON reg_data.pit_ticker = volume_day2_after.Symbol \n",
    "    AND reg_data.day_2_after = volume_day2_after.Date\n",
    "LEFT JOIN volume AS volume_day3_after\n",
    "    ON reg_data.pit_ticker = volume_day3_after.Symbol \n",
    "    AND reg_data.day_3_after = volume_day3_after.Date\n",
    "LEFT JOIN volume AS volume_day4_after\n",
    "    ON reg_data.pit_ticker = volume_day4_after.Symbol \n",
    "    AND reg_data.day_4_after = volume_day4_after.Date\n",
    "LEFT JOIN volume AS volume_day5_after\n",
    "    ON reg_data.pit_ticker = volume_day5_after.Symbol \n",
    "    AND reg_data.day_5_after = volume_day5_after.Date\n",
    "WHERE \n",
    "    volume_day_before.TotalVolume IS NOT NULL \n",
    "    AND volume_day_after.TotalVolume IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "\n",
    "# Scale shorting by total trading volume\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:02.686851900Z",
     "start_time": "2024-08-24T21:42:01.867952100Z"
    }
   },
   "id": "f0c20f895218c27e",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           CUSIP  permno  permco pit_ticker OFTIC             CNAME  \\\n0       73936310   84330   15207       PWAV  PWAV  POWERWAVE TECHNO   \n1       80603710   80362   12933       SCSC  SCSC    SCANSOURCE INC   \n2       20440W10   85602   32143        SID   SID      SID NACIONAL   \n3       40049110   79442   29916        SIM   SIM       SIMEC (GPO)   \n4       84546710   63765    4338        SWN   SWN  SOUTHWSTN ENERGY   \n...          ...     ...     ...        ...   ...               ...   \n187542  42726X10   90730   46854       HBOS  HBOS  HERITAGE FIN GRO   \n187543  74727D30   12493   15632       QADA  QADA           QAD INC   \n187544  45172L10   93209   53323        CTC   CTC   IFM INVESTMENTS   \n187545  59630420   14932   55027       MBCN  MBCN  MIDDLEFIELD BANC   \n187546  87655330   42219   21713       TSTY  TSTY   TASTY BAKING CO   \n\n                  ANNOUNCE  SURPRISE  POS_SURPRISE  pre_open  ...  \\\n0      2010-10-28 16:05:00  0.010582          True     False  ...   \n1      2010-10-28 16:05:00  0.002179          True     False  ...   \n2      2010-10-28 18:00:00 -0.013357         False     False  ...   \n3      2010-10-28 16:25:00 -0.088195         False     False  ...   \n4      2010-10-28 17:00:00  0.000306          True     False  ...   \n...                    ...       ...           ...       ...  ...   \n187542 2011-10-26 16:00:00  0.007456          True     False  ...   \n187543 2013-11-26 16:15:00  0.000651          True     False  ...   \n187544 2011-05-16 07:00:00 -0.015936         False      True  ...   \n187545 2017-02-07 16:05:00 -0.002308         False     False  ...   \n187546 2010-08-02 07:00:00 -0.034582         False      True  ...   \n\n        day5_before_volume day4_before_volume day3_before_volume  \\\n0                 176229.0          4987159.0           664917.0   \n1                  20156.0            28730.0            30871.0   \n2                1112641.0          1252919.0          1044743.0   \n3                   7600.0             6275.0             7377.0   \n4                 722492.0           635262.0          1133530.0   \n...                    ...                ...                ...   \n187542               600.0             1000.0              800.0   \n187543               910.0              600.0             1790.0   \n187544                 0.0              620.0            14650.0   \n187545                 0.0                0.0                0.0   \n187546              2700.0             3236.0             6429.0   \n\n       day2_before_volume day_before_volume day_after_volume  \\\n0                478094.0         1149767.0        2898097.0   \n1                103222.0           34335.0         134683.0   \n2               1440608.0          953591.0         760193.0   \n3                  6700.0           85207.0           7300.0   \n4               1296035.0          972049.0        2367553.0   \n...                   ...               ...              ...   \n187542             1000.0            1600.0           1200.0   \n187543             4750.0            5965.0           7791.0   \n187544             2300.0            3102.0           4300.0   \n187545                0.0             185.0             16.0   \n187546                0.0           32700.0           5450.0   \n\n       day2_after_volume day3_after_volume day4_after_volume day5_after_volume  \n0              1225914.0         1174824.0          784714.0         1317287.0  \n1                63442.0           13541.0            7046.0            7647.0  \n2               796310.0         1264570.0         1075372.0         1121395.0  \n3                 6075.0            6851.0            4950.0            5500.0  \n4              2818861.0         1289620.0         2037356.0         1220353.0  \n...                  ...               ...               ...               ...  \n187542            1900.0            1000.0           21100.0           10395.0  \n187543            1218.0           12202.0               0.0            4541.0  \n187544           12451.0           28550.0           27567.0            9829.0  \n187545               0.0               0.0               0.0             501.0  \n187546               0.0               0.0               0.0             446.0  \n\n[187547 rows x 70 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSIP</th>\n      <th>permno</th>\n      <th>permco</th>\n      <th>pit_ticker</th>\n      <th>OFTIC</th>\n      <th>CNAME</th>\n      <th>ANNOUNCE</th>\n      <th>SURPRISE</th>\n      <th>POS_SURPRISE</th>\n      <th>pre_open</th>\n      <th>...</th>\n      <th>day5_before_volume</th>\n      <th>day4_before_volume</th>\n      <th>day3_before_volume</th>\n      <th>day2_before_volume</th>\n      <th>day_before_volume</th>\n      <th>day_after_volume</th>\n      <th>day2_after_volume</th>\n      <th>day3_after_volume</th>\n      <th>day4_after_volume</th>\n      <th>day5_after_volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>73936310</td>\n      <td>84330</td>\n      <td>15207</td>\n      <td>PWAV</td>\n      <td>PWAV</td>\n      <td>POWERWAVE TECHNO</td>\n      <td>2010-10-28 16:05:00</td>\n      <td>0.010582</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>176229.0</td>\n      <td>4987159.0</td>\n      <td>664917.0</td>\n      <td>478094.0</td>\n      <td>1149767.0</td>\n      <td>2898097.0</td>\n      <td>1225914.0</td>\n      <td>1174824.0</td>\n      <td>784714.0</td>\n      <td>1317287.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80603710</td>\n      <td>80362</td>\n      <td>12933</td>\n      <td>SCSC</td>\n      <td>SCSC</td>\n      <td>SCANSOURCE INC</td>\n      <td>2010-10-28 16:05:00</td>\n      <td>0.002179</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>20156.0</td>\n      <td>28730.0</td>\n      <td>30871.0</td>\n      <td>103222.0</td>\n      <td>34335.0</td>\n      <td>134683.0</td>\n      <td>63442.0</td>\n      <td>13541.0</td>\n      <td>7046.0</td>\n      <td>7647.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20440W10</td>\n      <td>85602</td>\n      <td>32143</td>\n      <td>SID</td>\n      <td>SID</td>\n      <td>SID NACIONAL</td>\n      <td>2010-10-28 18:00:00</td>\n      <td>-0.013357</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>1112641.0</td>\n      <td>1252919.0</td>\n      <td>1044743.0</td>\n      <td>1440608.0</td>\n      <td>953591.0</td>\n      <td>760193.0</td>\n      <td>796310.0</td>\n      <td>1264570.0</td>\n      <td>1075372.0</td>\n      <td>1121395.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40049110</td>\n      <td>79442</td>\n      <td>29916</td>\n      <td>SIM</td>\n      <td>SIM</td>\n      <td>SIMEC (GPO)</td>\n      <td>2010-10-28 16:25:00</td>\n      <td>-0.088195</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>7600.0</td>\n      <td>6275.0</td>\n      <td>7377.0</td>\n      <td>6700.0</td>\n      <td>85207.0</td>\n      <td>7300.0</td>\n      <td>6075.0</td>\n      <td>6851.0</td>\n      <td>4950.0</td>\n      <td>5500.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84546710</td>\n      <td>63765</td>\n      <td>4338</td>\n      <td>SWN</td>\n      <td>SWN</td>\n      <td>SOUTHWSTN ENERGY</td>\n      <td>2010-10-28 17:00:00</td>\n      <td>0.000306</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>722492.0</td>\n      <td>635262.0</td>\n      <td>1133530.0</td>\n      <td>1296035.0</td>\n      <td>972049.0</td>\n      <td>2367553.0</td>\n      <td>2818861.0</td>\n      <td>1289620.0</td>\n      <td>2037356.0</td>\n      <td>1220353.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>187542</th>\n      <td>42726X10</td>\n      <td>90730</td>\n      <td>46854</td>\n      <td>HBOS</td>\n      <td>HBOS</td>\n      <td>HERITAGE FIN GRO</td>\n      <td>2011-10-26 16:00:00</td>\n      <td>0.007456</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>600.0</td>\n      <td>1000.0</td>\n      <td>800.0</td>\n      <td>1000.0</td>\n      <td>1600.0</td>\n      <td>1200.0</td>\n      <td>1900.0</td>\n      <td>1000.0</td>\n      <td>21100.0</td>\n      <td>10395.0</td>\n    </tr>\n    <tr>\n      <th>187543</th>\n      <td>74727D30</td>\n      <td>12493</td>\n      <td>15632</td>\n      <td>QADA</td>\n      <td>QADA</td>\n      <td>QAD INC</td>\n      <td>2013-11-26 16:15:00</td>\n      <td>0.000651</td>\n      <td>True</td>\n      <td>False</td>\n      <td>...</td>\n      <td>910.0</td>\n      <td>600.0</td>\n      <td>1790.0</td>\n      <td>4750.0</td>\n      <td>5965.0</td>\n      <td>7791.0</td>\n      <td>1218.0</td>\n      <td>12202.0</td>\n      <td>0.0</td>\n      <td>4541.0</td>\n    </tr>\n    <tr>\n      <th>187544</th>\n      <td>45172L10</td>\n      <td>93209</td>\n      <td>53323</td>\n      <td>CTC</td>\n      <td>CTC</td>\n      <td>IFM INVESTMENTS</td>\n      <td>2011-05-16 07:00:00</td>\n      <td>-0.015936</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>620.0</td>\n      <td>14650.0</td>\n      <td>2300.0</td>\n      <td>3102.0</td>\n      <td>4300.0</td>\n      <td>12451.0</td>\n      <td>28550.0</td>\n      <td>27567.0</td>\n      <td>9829.0</td>\n    </tr>\n    <tr>\n      <th>187545</th>\n      <td>59630420</td>\n      <td>14932</td>\n      <td>55027</td>\n      <td>MBCN</td>\n      <td>MBCN</td>\n      <td>MIDDLEFIELD BANC</td>\n      <td>2017-02-07 16:05:00</td>\n      <td>-0.002308</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>185.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>501.0</td>\n    </tr>\n    <tr>\n      <th>187546</th>\n      <td>87655330</td>\n      <td>42219</td>\n      <td>21713</td>\n      <td>TSTY</td>\n      <td>TSTY</td>\n      <td>TASTY BAKING CO</td>\n      <td>2010-08-02 07:00:00</td>\n      <td>-0.034582</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2700.0</td>\n      <td>3236.0</td>\n      <td>6429.0</td>\n      <td>0.0</td>\n      <td>32700.0</td>\n      <td>5450.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>446.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>187547 rows × 70 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:02.829534Z",
     "start_time": "2024-08-24T21:43:02.701250200Z"
    }
   },
   "id": "25664974a91fd41f",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                    0\n0               CUSIP\n1              permno\n2              permco\n3          pit_ticker\n4               OFTIC\n..                ...\n65   day_after_volume\n66  day2_after_volume\n67  day3_after_volume\n68  day4_after_volume\n69  day5_after_volume\n\n[70 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CUSIP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>permno</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>permco</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pit_ticker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>OFTIC</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>day_after_volume</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>day2_after_volume</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>day3_after_volume</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>day4_after_volume</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>day5_after_volume</td>\n    </tr>\n  </tbody>\n</table>\n<p>70 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reg_data.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:02.848957100Z",
     "start_time": "2024-08-24T21:43:02.827382400Z"
    }
   },
   "id": "175b5e05b82a3459",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Daily variable preparation\n",
    "\n",
    "# Cumulative daily shorting before\n",
    "for i in range(2, 6):\n",
    "    cols = ['day_be_shorts'] + [f'day{j}_be_shorts' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_be_shorts'] = reg_data[cols].sum(axis=1) / i\n",
    "\n",
    "# Cumulative daily shorting after\n",
    "for i in range(2, 6):\n",
    "    cols = ['day_af_shorts'] + [f'day{j}_af_shorts' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_af_shorts'] = reg_data[cols].sum(axis=1) / i\n",
    "\n",
    "# Cumulative off-exchange total volume\n",
    "for i in range(2, 6):\n",
    "    cols = ['day_before_volume'] + [f'day{j}_before_volume' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_before_volume'] = (reg_data[cols].sum(axis=1) / i) + 1\n",
    "    cols = ['day_after_volume'] + [f'day{j}_after_volume' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_after_volume'] = (reg_data[cols].sum(axis=1) / i) + 1\n",
    "\n",
    "# Cumulative on-exchange total volume\n",
    "for i in range(2, 6):\n",
    "    cols = ['day_before_volume_crsp'] + [f'day{j}_before_volume_crsp' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_before_volume_crsp'] = (reg_data[cols].sum(axis=1) / i) + 1\n",
    "    cols = ['day_after_volume_crsp'] + [f'day{j}_after_volume_crsp' for j in range(2, i + 1)]\n",
    "    reg_data[f'cumu{i}_after_volume_crsp'] = (reg_data[cols].sum(axis=1) / i) + 1\n",
    "\n",
    "cols = [f'day{j}_before_volume' for j in range(2, 7)]\n",
    "reg_data['wk_before_volume'] = (reg_data[cols].sum(axis=1) / 5) + 1\n",
    "cols = [f'day{j}_before_volume_crsp' for j in range(2, 7)]\n",
    "reg_data['wk_before_volume_crsp'] = (reg_data[cols].sum(axis=1) / 5) + 1\n",
    "\n",
    "reg_data['avg_month_before_shorts'] += 1\n",
    "\n",
    "# Scaled shorts calculations\n",
    "reg_data['day_be_shorts_sc'] = reg_data['day_be_shorts'] / (reg_data['day_before_volume'] + 1)\n",
    "reg_data['day_af_shorts_sc'] = reg_data['day_af_shorts'] / (reg_data['day_after_volume'] + 1)\n",
    "reg_data['day_be_shorts_sc_cr'] = reg_data['day_be_shorts'] / (reg_data['day_before_volume_crsp'] + 1)\n",
    "reg_data['day_af_shorts_sc_cr'] = reg_data['day_af_shorts'] / (reg_data['day_after_volume_crsp'] + 1)\n",
    "reg_data['day_be_shorts_sc_wk'] = reg_data['day_be_shorts'] / reg_data['wk_before_volume']\n",
    "reg_data['day_af_shorts_sc_wk'] = reg_data['day_af_shorts'] / reg_data['wk_before_volume']\n",
    "reg_data['day_be_shorts_sc_cr_wk'] = reg_data['day_be_shorts'] / reg_data['wk_before_volume_crsp']\n",
    "reg_data['day_af_shorts_sc_cr_wk'] = reg_data['day_af_shorts'] / reg_data['wk_before_volume_crsp']\n",
    "reg_data['day_be_shorts_mo'] = reg_data['day_be_shorts'] / reg_data['avg_month_before_shorts']\n",
    "reg_data['day_af_shorts_mo'] = reg_data['day_af_shorts'] / reg_data['avg_month_before_shorts']\n",
    "\n",
    "for i in range(2, 6):\n",
    "    # Off-exchange volume (same window)\n",
    "    reg_data[f'cumu{i}_be_shorts_sc'] = reg_data[f'cumu{i}_be_shorts'] / reg_data[f'cumu{i}_before_volume']\n",
    "    reg_data[f'cumu{i}_af_shorts_sc'] = reg_data[f'cumu{i}_af_shorts'] / reg_data[f'cumu{i}_after_volume']\n",
    "    # On-exchange volume (same window)\n",
    "    reg_data[f'cumu{i}_be_shorts_sc_cr'] = reg_data[f'cumu{i}_be_shorts'] / reg_data[f'cumu{i}_before_volume_crsp']\n",
    "    reg_data[f'cumu{i}_af_shorts_sc_cr'] = reg_data[f'cumu{i}_af_shorts'] / reg_data[f'cumu{i}_after_volume_crsp']\n",
    "    # Off-exchange volume week before\n",
    "    reg_data[f'cumu{i}_be_shorts_sc_wk'] = reg_data[f'cumu{i}_be_shorts'] / reg_data['wk_before_volume']\n",
    "    reg_data[f'cumu{i}_af_shorts_sc_wk'] = reg_data[f'cumu{i}_af_shorts'] / reg_data['wk_before_volume']\n",
    "    # On-exchange volume week before\n",
    "    reg_data[f'cumu{i}_be_shorts_sc_cr_wk'] = reg_data[f'cumu{i}_be_shorts'] / reg_data['wk_before_volume_crsp']\n",
    "    reg_data[f'cumu{i}_af_shorts_sc_cr_wk'] = reg_data[f'cumu{i}_af_shorts'] / reg_data['wk_before_volume_crsp']\n",
    "    # Monthly shorting\n",
    "    reg_data[f'cumu{i}_be_shorts_mo'] = reg_data[f'cumu{i}_be_shorts'] / reg_data['avg_month_before_shorts']\n",
    "    reg_data[f'cumu{i}_af_shorts_mo'] = reg_data[f'cumu{i}_af_shorts'] / reg_data['avg_month_before_shorts']\n",
    "\n",
    "reg_data = reg_data[(reg_data['day_be_shorts_sc'] <= 1) & (reg_data['day_af_shorts_sc'] <= 1)] # Drop shorts/off-exchange total greater than 100%\n",
    "reg_data = reg_data[(reg_data['cumu2_be_shorts_sc'] <= 1) & (reg_data['cumu2_af_shorts_sc'] <= 1)]\n",
    "reg_data = reg_data[(reg_data['cumu3_be_shorts_sc'] <= 1) & (reg_data['cumu3_af_shorts_sc'] <= 1)]\n",
    "reg_data = reg_data[(reg_data['cumu4_be_shorts_sc'] <= 1) & (reg_data['cumu4_af_shorts_sc'] <= 1)]\n",
    "reg_data = reg_data[(reg_data['cumu5_be_shorts_sc'] <= 1) & (reg_data['cumu5_af_shorts_sc'] <= 1)]\n",
    "\n",
    "# Prepare for STATA\n",
    "\n",
    "# Year-quarter time variable\n",
    "reg_data.loc[:, 'year'] = reg_data['ANNOUNCE'].dt.year\n",
    "reg_data.loc[:, 'quarter'] = reg_data['ANNOUNCE'].dt.quarter\n",
    "reg_data.loc[:,'year_quarter'] = (reg_data['year'] - 2010) * 4 + reg_data['quarter']\n",
    "\n",
    "# Drop duplicates, keeping the later EA\n",
    "reg_data = reg_data.sort_values(by=['CUSIP', 'year_quarter', 'ANNOUNCE'])\n",
    "reg_data = reg_data.drop_duplicates(subset=['CUSIP', 'year_quarter'], keep='last')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:05.100410900Z",
     "start_time": "2024-08-24T21:43:02.858458Z"
    }
   },
   "id": "a935ca0a653da636",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reg_data.to_csv(working_folder + '/reg_data.txt', index=False)\n",
    "reg_data.to_stata(working_folder + '/reg_data.dta')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:25.693595400Z",
     "start_time": "2024-08-24T21:43:05.097414100Z"
    }
   },
   "id": "f20e4356fb07f76e",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for year in range(2010, 2024):\n",
    "    file_path = f'{working_folder}/FINRA_30min_Agg/agg_{year}_30min.pkl'\n",
    "\n",
    "    # Read the data into a dataframe with the same name as the file\n",
    "    globals()[f'agg_{year}_30min'] = pd.read_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:04:58.918558900Z",
     "start_time": "2024-08-13T15:04:20.475547300Z"
    }
   },
   "id": "5278a384cf69216a",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load in data from each year 2010-2023\n",
    "# Load aggregate short files for speed\n",
    "for year in range(2010, 2024):\n",
    "    file_path = f'{working_folder}/FINRA_1hr_Agg/agg_{year}_1hr.pkl'\n",
    "\n",
    "    # Read the data into a dataframe with the same name as the file\n",
    "    globals()[f'agg_{year}_1hr'] = pd.read_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-23T15:28:39.171715Z",
     "start_time": "2024-08-23T15:28:26.331240900Z"
    }
   },
   "id": "ecce2888ac499dce",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9a072bcda3b4b7fbb0d4f92232af55d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Day before hourly shorts\n",
    "# Consider dropping rows that are missing for trading hours and only have non trading hours\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('reg_data', reg_data)\n",
    "\n",
    "# Adjust the query to match on the hour as well\n",
    "query = f\"\"\"\n",
    "WITH short_data AS (\n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2010_1hr UNION ALL\n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2011_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2012_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2013_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2014_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2015_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2016_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2017_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2018_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2019_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2020_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2021_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2022_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2023_1hr\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    reg_data.*, \n",
    "    {', '.join([f'COALESCE(short_data_day_before_{i}.Shorts, 0) AS be_shorts_hr{i}' for i in range(24)])},\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN short_data AS short_data_day_before_{0} ON reg_data.pit_ticker = short_data_day_before_{0}.Symbol AND reg_data.day_before = short_data_day_before_{0}.Date AND short_data_day_before_{0}.Hour = 0\n",
    "{''.join([f'LEFT JOIN short_data AS short_data_day_before_{i} ON reg_data.pit_ticker = short_data_day_before_{i}.Symbol AND reg_data.day_before = short_data_day_before_{i}.Date AND short_data_day_before_{i}.Hour = {i} ' for i in range(1, 24)])}\n",
    "ORDER BY \n",
    "    reg_data.ANNOUNCE, \n",
    "    reg_data.pit_ticker\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data_hourly = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:37:45.361433Z",
     "start_time": "2024-08-22T23:26:34.512727700Z"
    }
   },
   "id": "c3b8abee96bec4f9",
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cdda5d6ae544025b833be1ca388fccd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Day after hourly shorts\n",
    "# Need to modify so that if shorts are missing for day before and day after, all times, the row is deleted\n",
    "# Consider dropping rows that are missing for trading hours and only have non trading hours\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('reg_data_hourly', reg_data_hourly)\n",
    "\n",
    "# Adjust the query to match on the hour as well\n",
    "query = f\"\"\"\n",
    "WITH short_data AS (\n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2010_1hr UNION ALL\n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2011_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2012_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2013_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2014_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2015_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2016_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2017_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2018_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2019_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2020_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2021_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2022_1hr UNION ALL \n",
    "    SELECT Symbol, Date, Shorts, Hour\n",
    "    FROM agg_2023_1hr\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    reg_data_hourly.*, \n",
    "    {', '.join([f'COALESCE(short_data_day_after_{i}.Shorts, 0) AS af_shorts_hr{i}' for i in range(24)])},\n",
    "FROM\n",
    "    reg_data_hourly\n",
    "LEFT JOIN short_data AS short_data_day_after_{0} ON reg_data_hourly.pit_ticker = short_data_day_after_{0}.Symbol AND reg_data_hourly.day_before = short_data_day_after_{0}.Date AND short_data_day_after_{0}.Hour = 0\n",
    "{''.join([f'LEFT JOIN short_data AS short_data_day_after_{i} ON reg_data_hourly.pit_ticker = short_data_day_after_{i}.Symbol AND reg_data_hourly.day_after = short_data_day_after_{i}.Date AND short_data_day_after_{i}.Hour = {i} ' for i in range(1, 24)])}\n",
    "ORDER BY \n",
    "    reg_data_hourly.ANNOUNCE, \n",
    "    reg_data_hourly.pit_ticker\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data_hourly = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:49:13.203111800Z",
     "start_time": "2024-08-22T23:37:45.376959800Z"
    }
   },
   "id": "93a22119d370de88",
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T17:02:11.963113700Z",
     "start_time": "2024-08-13T17:02:11.874596600Z"
    }
   },
   "id": "b8b978e0a121a62c",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "68d0534089492978",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reg_data = pd.read_csv(working_folder + '/reg_data.txt', parse_dates=['ANNOUNCE', 'day_25_before', 'day_6_before', 'day_5_before', 'day_4_before', 'day_3_before', 'day_2_before', 'day_before', 'day_after', 'day_2_after','day_3_after','day_4_after','day_5_after'], date_format='ISO8601')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:49:21.206203700Z",
     "start_time": "2024-08-22T23:49:13.201956100Z"
    }
   },
   "id": "c9e6029b239a46ba",
   "execution_count": 162
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed0b5e8fd73148c59312e96ebd994684"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = reg_data_hourly.columns\n",
    "\n",
    "# Filter columns that start with 'be_shorts_hr' or 'af_shorts_hr'\n",
    "day_before_columns = [col for col in columns if col.startswith('be_shorts_hr')]\n",
    "day_after_columns = [col for col in columns if col.startswith('af_shorts_hr')]\n",
    "\n",
    "select_columns = ', '.join(day_before_columns + day_after_columns)\n",
    "\n",
    "# Create a connection to an in-memory DuckDB instance\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register('reg_data', reg_data)\n",
    "con.register('reg_data_hourly', reg_data_hourly)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    reg_data.*, {select_columns}\n",
    "FROM\n",
    "    reg_data\n",
    "LEFT JOIN reg_data_hourly\n",
    "    ON reg_data.pit_ticker = reg_data_hourly.pit_ticker \n",
    "    AND reg_data.ANNOUNCE = reg_data_hourly.ANNOUNCE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query \n",
    "reg_data_daily_hourly = con.execute(query).df()\n",
    "\n",
    "# Close the connection\n",
    "\n",
    "# Scale shorting by total trading volume\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:49:24.473809200Z",
     "start_time": "2024-08-22T23:49:21.209652200Z"
    }
   },
   "id": "6fe18dc1306843e9",
   "execution_count": 163
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = reg_data_daily_hourly.columns\n",
    "\n",
    "# Filter columns that start with 'day_be_shorts_' or 'day_af_shorts_'\n",
    "columns = [col for col in columns if (col.startswith('be_shorts_hr') | col.startswith('af_shorts_hr'))]\n",
    "\n",
    "for col in columns:\n",
    "    if col.startswith('be_shorts_'):\n",
    "        reg_data_daily_hourly[col+'_sc'] = reg_data_daily_hourly[col] / (reg_data_daily_hourly['day_before_volume'] + 1)\n",
    "        reg_data_daily_hourly[col+'_sc_cr'] = reg_data_daily_hourly[col] / (reg_data_daily_hourly['day_before_volume_crsp'] + 1)\n",
    "    if col.startswith('af_shorts_'):\n",
    "        reg_data_daily_hourly[col+'_sc'] = reg_data_daily_hourly[col] / (reg_data_daily_hourly['day_after_volume'] + 1)\n",
    "        reg_data_daily_hourly[col+'_sc_cr'] = reg_data_daily_hourly[col] / (reg_data_daily_hourly['day_after_volume_crsp'] + 1)\n",
    "    reg_data_daily_hourly[col+'_sc_wk'] = reg_data_daily_hourly[col] / reg_data_daily_hourly['wk_before_volume']\n",
    "    reg_data_daily_hourly[col+'_sc_wk_cr'] = reg_data_daily_hourly[col] / reg_data_daily_hourly['wk_before_volume_crsp']\n",
    "    reg_data_daily_hourly[col+'_mo'] = reg_data_daily_hourly[col] / reg_data_daily_hourly['avg_month_before_shorts']\n",
    "\n",
    "reg_data_daily_hourly = reg_data_daily_hourly.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8243125c384fae14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reg_data_daily_hourly.to_csv(working_folder + '/reg_data_daily_hourly.txt', index=False)\n",
    "reg_data_daily_hourly.to_stata(working_folder + '/reg_data_daily_hourly.dta')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:50:18.492648600Z",
     "start_time": "2024-08-22T23:49:25.226280600Z"
    }
   },
   "id": "ee5cf41eb843169d",
   "execution_count": 165
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                           0\n0                      CUSIP\n1                     permno\n2                     permco\n3                 pit_ticker\n4                      CNAME\n..                       ...\n428        af_shorts_hr23_sc\n429     af_shorts_hr23_sc_cr\n430     af_shorts_hr23_sc_wk\n431  af_shorts_hr23_sc_wk_cr\n432        af_shorts_hr23_mo\n\n[433 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CUSIP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>permno</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>permco</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pit_ticker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CNAME</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>af_shorts_hr23_sc</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>af_shorts_hr23_sc_cr</td>\n    </tr>\n    <tr>\n      <th>430</th>\n      <td>af_shorts_hr23_sc_wk</td>\n    </tr>\n    <tr>\n      <th>431</th>\n      <td>af_shorts_hr23_sc_wk_cr</td>\n    </tr>\n    <tr>\n      <th>432</th>\n      <td>af_shorts_hr23_mo</td>\n    </tr>\n  </tbody>\n</table>\n<p>433 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reg_data_daily_hourly.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:50:18.509825300Z",
     "start_time": "2024-08-22T23:50:18.492648600Z"
    }
   },
   "id": "44b6aa1356a567d3",
   "execution_count": 166
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           CUSIP  permno  permco pit_ticker         CNAME            ANNOUNCE  \\\n0       00030710   14945   55038        AAC  AAC HOLDINGS 2015-04-29 16:05:00   \n1       00030710   14945   55038        AAC     AAC HOLDG 2015-10-27 17:15:00   \n2       00030710   14945   55038        AAC     AAC HOLDG 2016-02-23 06:30:00   \n3       00030710   14945   55038        AAC     AAC HOLDG 2016-05-05 06:30:00   \n4       00030710   14945   55038        AAC     AAC HOLDG 2016-08-04 06:30:00   \n...          ...     ...     ...        ...           ...                 ...   \n178252  Y9530810   15857   55491        WVE     WAVE LIFE 2019-03-01 07:30:00   \n178253  Y9530810   15857   55491        WVE     WAVE LIFE 2019-05-10 07:30:00   \n178254  Y9530810   15857   55491        WVE     WAVE LIFE 2020-05-11 07:30:00   \n178255  Y9530810   15857   55491        WVE     WAVE LIFE 2020-08-10 07:45:00   \n178256  Y9530810   15857   55491        WVE     WAVE LIFE 2021-03-04 07:00:00   \n\n           VALUE    MEDEST  SURPRISE  POS_SURPRISE  ...  af_shorts_hr22_sc  \\\n0       0.004511  0.003008  0.001504          True  ...                0.0   \n1       0.008562  0.005708  0.002854          True  ...                0.0   \n2       0.008445  0.008942 -0.000497         False  ...                0.0   \n3       0.009775  0.007820  0.001955          True  ...                0.0   \n4       0.007653  0.009779 -0.002126         False  ...                0.0   \n...          ...       ...       ...           ...  ...                ...   \n178252 -0.034613 -0.033532 -0.001082         False  ...                0.0   \n178253 -0.049151 -0.045175 -0.003975         False  ...                0.0   \n178254 -0.166667 -0.159420 -0.007246         False  ...                0.0   \n178255 -0.130979 -0.112756 -0.018223         False  ...                0.0   \n178256 -0.061077 -0.060041 -0.001035         False  ...                0.0   \n\n        af_shorts_hr22_sc_cr af_shorts_hr22_sc_wk af_shorts_hr22_sc_wk_cr  \\\n0                        0.0                  0.0                     0.0   \n1                        0.0                  0.0                     0.0   \n2                        0.0                  0.0                     0.0   \n3                        0.0                  0.0                     0.0   \n4                        0.0                  0.0                     0.0   \n...                      ...                  ...                     ...   \n178252                   0.0                  0.0                     0.0   \n178253                   0.0                  0.0                     0.0   \n178254                   0.0                  0.0                     0.0   \n178255                   0.0                  0.0                     0.0   \n178256                   0.0                  0.0                     0.0   \n\n       af_shorts_hr22_mo af_shorts_hr23_sc af_shorts_hr23_sc_cr  \\\n0                    0.0               0.0                  0.0   \n1                    0.0               0.0                  0.0   \n2                    0.0               0.0                  0.0   \n3                    0.0               0.0                  0.0   \n4                    0.0               0.0                  0.0   \n...                  ...               ...                  ...   \n178252               0.0               0.0                  0.0   \n178253               0.0               0.0                  0.0   \n178254               0.0               0.0                  0.0   \n178255               0.0               0.0                  0.0   \n178256               0.0               0.0                  0.0   \n\n       af_shorts_hr23_sc_wk af_shorts_hr23_sc_wk_cr af_shorts_hr23_mo  \n0                       0.0                     0.0               0.0  \n1                       0.0                     0.0               0.0  \n2                       0.0                     0.0               0.0  \n3                       0.0                     0.0               0.0  \n4                       0.0                     0.0               0.0  \n...                     ...                     ...               ...  \n178252                  0.0                     0.0               0.0  \n178253                  0.0                     0.0               0.0  \n178254                  0.0                     0.0               0.0  \n178255                  0.0                     0.0               0.0  \n178256                  0.0                     0.0               0.0  \n\n[178257 rows x 433 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSIP</th>\n      <th>permno</th>\n      <th>permco</th>\n      <th>pit_ticker</th>\n      <th>CNAME</th>\n      <th>ANNOUNCE</th>\n      <th>VALUE</th>\n      <th>MEDEST</th>\n      <th>SURPRISE</th>\n      <th>POS_SURPRISE</th>\n      <th>...</th>\n      <th>af_shorts_hr22_sc</th>\n      <th>af_shorts_hr22_sc_cr</th>\n      <th>af_shorts_hr22_sc_wk</th>\n      <th>af_shorts_hr22_sc_wk_cr</th>\n      <th>af_shorts_hr22_mo</th>\n      <th>af_shorts_hr23_sc</th>\n      <th>af_shorts_hr23_sc_cr</th>\n      <th>af_shorts_hr23_sc_wk</th>\n      <th>af_shorts_hr23_sc_wk_cr</th>\n      <th>af_shorts_hr23_mo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC HOLDINGS</td>\n      <td>2015-04-29 16:05:00</td>\n      <td>0.004511</td>\n      <td>0.003008</td>\n      <td>0.001504</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2015-10-27 17:15:00</td>\n      <td>0.008562</td>\n      <td>0.005708</td>\n      <td>0.002854</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-02-23 06:30:00</td>\n      <td>0.008445</td>\n      <td>0.008942</td>\n      <td>-0.000497</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-05-05 06:30:00</td>\n      <td>0.009775</td>\n      <td>0.007820</td>\n      <td>0.001955</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00030710</td>\n      <td>14945</td>\n      <td>55038</td>\n      <td>AAC</td>\n      <td>AAC HOLDG</td>\n      <td>2016-08-04 06:30:00</td>\n      <td>0.007653</td>\n      <td>0.009779</td>\n      <td>-0.002126</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>178252</th>\n      <td>Y9530810</td>\n      <td>15857</td>\n      <td>55491</td>\n      <td>WVE</td>\n      <td>WAVE LIFE</td>\n      <td>2019-03-01 07:30:00</td>\n      <td>-0.034613</td>\n      <td>-0.033532</td>\n      <td>-0.001082</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>178253</th>\n      <td>Y9530810</td>\n      <td>15857</td>\n      <td>55491</td>\n      <td>WVE</td>\n      <td>WAVE LIFE</td>\n      <td>2019-05-10 07:30:00</td>\n      <td>-0.049151</td>\n      <td>-0.045175</td>\n      <td>-0.003975</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>178254</th>\n      <td>Y9530810</td>\n      <td>15857</td>\n      <td>55491</td>\n      <td>WVE</td>\n      <td>WAVE LIFE</td>\n      <td>2020-05-11 07:30:00</td>\n      <td>-0.166667</td>\n      <td>-0.159420</td>\n      <td>-0.007246</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>178255</th>\n      <td>Y9530810</td>\n      <td>15857</td>\n      <td>55491</td>\n      <td>WVE</td>\n      <td>WAVE LIFE</td>\n      <td>2020-08-10 07:45:00</td>\n      <td>-0.130979</td>\n      <td>-0.112756</td>\n      <td>-0.018223</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>178256</th>\n      <td>Y9530810</td>\n      <td>15857</td>\n      <td>55491</td>\n      <td>WVE</td>\n      <td>WAVE LIFE</td>\n      <td>2021-03-04 07:00:00</td>\n      <td>-0.061077</td>\n      <td>-0.060041</td>\n      <td>-0.001035</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>178257 rows × 433 columns</p>\n</div>"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data_daily_hourly"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T23:50:18.642401200Z",
     "start_time": "2024-08-22T23:50:18.504566500Z"
    }
   },
   "id": "75fde883bbd4a3cc",
   "execution_count": 167
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STATA Commands"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0ae1963cfead75"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_file = f\"{working_folder}/STATA_vars_original.txt\"\n",
    "output_file = f\"{working_folder}/STATA_vars_clean.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:48:19.520942300Z",
     "start_time": "2024-08-24T21:48:19.468290200Z"
    }
   },
   "id": "76c13e8ef698551b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File paths\n",
    "input_file = f\"{working_folder}/STATA_vars_original.txt\"\n",
    "output_file = f\"{working_folder}/STATA_vars_clean.txt\"\n",
    "\n",
    "# Reading the input file\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Preparing the output\n",
    "output_lines = []\n",
    "current_command = \"\"\n",
    "varlist = []\n",
    "current_varlist_name = \"\"\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:  # Skip empty lines\n",
    "        continue\n",
    "    if line.endswith(':'):\n",
    "        if current_command:\n",
    "            # Winsorize command\n",
    "            output_lines.append(f\"winsor2 {current_command.strip()}\\n\")\n",
    "            # Normalize command\n",
    "            multiplied_vars = []\n",
    "            for var in varlist:\n",
    "                if len(var) >= 29:\n",
    "                    print(f\"{var} is {len(var)} characters\\n\")\n",
    "                winsorized_var = f\"{var}_w\"\n",
    "                normalized_var = f\"{winsorized_var}z\"\n",
    "                multiplied_var = f\"{winsorized_var}m\"\n",
    "                output_lines.append(f\"egen {normalized_var} = std({winsorized_var})\\n\")\n",
    "                output_lines.append(f\"gen {multiplied_var} = {normalized_var} / 100\\n\")\n",
    "                multiplied_vars.append(multiplied_var)\n",
    "            # Varlist creation with winsorized and normalized variables\n",
    "            output_lines.append(f\"vl create {current_varlist_name} = ({' '.join(multiplied_vars)})\\n\")\n",
    "        current_command = \"\"\n",
    "        varlist = []\n",
    "        current_varlist_name = line[:-1]  # Remove the colon to get the varlist name\n",
    "    else:\n",
    "        current_command += line + \" \"\n",
    "        varlist.append(line)\n",
    "\n",
    "if current_command:\n",
    "    # Winsorize command\n",
    "    output_lines.append(f\"winsor2 {current_command.strip()}\\n\")\n",
    "    # Normalize command\n",
    "    multiplied_vars = []\n",
    "    for var in varlist:\n",
    "        if len(var) >= 29:\n",
    "            output_lines.append(f\"{var} is {len(var)} characters\\n\")\n",
    "        winsorized_var = f\"{var}_w\"\n",
    "        normalized_var = f\"{winsorized_var}z\"\n",
    "        multiplied_var = f\"{winsorized_var}m\"\n",
    "        output_lines.append(f\"egen {normalized_var} = std({winsorized_var})\\n\")\n",
    "        output_lines.append(f\"gen {multiplied_var} = {normalized_var} / 100\\n\")\n",
    "        multiplied_vars.append(multiplied_var)\n",
    "    # Varlist creation with winsorized and normalized variables\n",
    "    output_lines.append(f\"vl create {current_varlist_name} = ({' '.join(multiplied_vars)})\\n\")\n",
    "\n",
    "# Writing to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    file.writelines(output_lines)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T21:48:22.704663400Z",
     "start_time": "2024-08-24T21:48:22.686615200Z"
    }
   },
   "id": "16fd48b905eee151",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
